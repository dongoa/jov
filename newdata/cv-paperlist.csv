Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier,Venues
Offline Signature Verification Using Online Handwriting Registration, Yu Qiao; Jianzhuang Liu; Xiaoou Tang,"Department of Information Engineering, The Chinese University of Hong Kong, qiao@gavo.t.u-tokyo.ac.jp; Department of Information Engineering, The Chinese University of Hong Kong, jzliu@ie.cuhk.edu.hk; Microsoft Research Asia, Beijing, China. xitang@microsoft.com",2007 IEEE Conference on Computer Vision and Pattern Recognition,,2007,,,1,8,"This paper proposes a novel framework for offline signature verification. Different from previous methods, our approach makes use of online handwriting instead of handwritten images for registration. The online registrations enable robust recovery of the writing trajectory from an input offline signature and thus allow effective shape matching between registration and verification signatures. In addition, we propose several new techniques to improve the performance of the new signature verification system: 1. we formulate and solve the recovery of writing trajectory within the framework of conditional random fields; 2. we propose a new shape descriptor, online context, for aligning signatures; 3. we develop a verification criterion which combines the duration and amplitude variances of handwriting. Experiments on a benchmark database show that the proposed method significantly outperforms the well-known offline signature verification methods and achieve comparable performance with online signature verification methods.",,,10.1109/CVPR.2007.383263,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270288,,Handwriting recognition;Writing;Shape;Feature extraction;Data mining;Asia;Robustness;Databases;Art;Error analysis,handwriting recognition;image matching;image registration,offline signature verification;online handwriting registration;handwritten images;shape matching,,66,,26,,,,,IEEE,IEEE Conferences,CVPR
Motionlets: Mid-level 3D Parts for Human Motion Recognition, Limin Wang; Yu Qiao; Xiaoou Tang,"Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China",2013 IEEE Conference on Computer Vision and Pattern Recognition,,2013,,,2674,2681,"This paper proposes motionlet, a mid-level and spatiotemporal part, for human motion recognition. Motion let can be seen as a tight cluster in motion and appearance space, corresponding to the moving process of different body parts. We postulate three key properties of motion let for action recognition: high motion saliency, multiple scale representation, and representative-discriminative ability. Towards this goal, we develop a data-driven approach to learn motion lets from training videos. First, we extract 3D regions with high motion saliency. Then we cluster these regions and preserve the centers as candidate templates for motion let. Finally, we examine the representative and discriminative power of the candidates, and introduce a greedy method to select effective candidates. With motion lets, we present a mid-level representation for video, called motionlet activation vector. We conduct experiments on three datasets, KTH, HMDB51, and UCF50. The results show that the proposed methods significantly outperform state-of-the-art methods.",,,10.1109/CVPR.2013.345,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619189,,Videos;Three-dimensional displays;Feature extraction;Spatiotemporal phenomena;Histograms;Vectors;Detectors,computer graphics;feature extraction;image motion analysis;video signal processing,mid-level 3D parts;human motion recognition;spatiotemporal part;body parts;action recognition;high motion saliency;multiple scale representation;representative-discriminative ability;data-driven approach;training videos;3D region extraction;motionlet activation vector;KTH dataset;HMDB51 dataset;UCF50 dataset,,187,,32,,,,,IEEE,IEEE Conferences,CVPR
Multi-view Super Vector for Action Recognition, Zhuowei Cai; Limin Wang; Xiaojiang Peng; Yu Qiao,"Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China",2014 IEEE Conference on Computer Vision and Pattern Recognition,,2014,,,596,603,"Images and videos are often characterized by multiple types of local descriptors such as SIFT, HOG and HOF, each of which describes certain aspects of object feature. Recognition systems benefit from fusing multiple types of these descriptors. Two widely applied fusion pipelines are descriptor concatenation and kernel average. The first one is effective when different descriptors are strongly correlated, while the second one is probably better when descriptors are relatively independent. In practice, however, different descriptors are neither fully independent nor fully correlated, and previous fusion methods may not be satisfying. In this paper, we propose a new global representation, Multi-View Super Vector (MVSV), which is composed of relatively independent components derived from a pair of descriptors. Kernel average is then applied on these components to produce recognition result. To obtain MVSV, we develop a generative mixture model of probabilistic canonical correlation analyzers (M-PCCA), and utilize the hidden factors and gradient vectors of M-PCCA to construct MVSV for video representation. Experiments on video based action recognition tasks show that MVSV achieves promising results, and outperforms FV and VLAD with descriptor concatenation or kernel average fusion strategy.",,,10.1109/CVPR.2014.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909477,multi-view;action recognition;mixture model;canonical correlation analysis,Vectors;Videos;Kernel;Correlation;Probabilistic logic;Accuracy;Encoding,feature extraction;image motion analysis;image representation;object recognition;statistical analysis;video signal processing,multiview super vector;SIFT descriptors;HOG descriptors;HOF descriptors;scale invariant feature transforms;histogram-of-oriented gradients;object feature;fusion pipelines;descriptor concatenation;kernel average;MVSV representation;M-PCCA;generative mixture model of probabilistic canonical correlation analyzers;video based action recognition tasks,,154,,39,,,,,IEEE,IEEE Conferences,CVPR
Action recognition with trajectory-pooled deep-convolutional descriptors,Limin Wang; Yu Qiao; Xiaoou Tang,"Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong",2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2015,,,4305,4314,"Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.",,,10.1109/CVPR.2015.7299059,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299059,,Trajectory;Feature extraction;Videos;Optical imaging;Spatiotemporal phenomena;Visualization;Three-dimensional displays,convolution;learning (artificial intelligence);video signal processing,action recognition;trajectory-pooled deep-convolutional descriptors;TDD;visual features;human action understanding;video representation;hand-crafted features;deep-learned features;deep architectures;discriminative convolutional feature maps;spatiotemporal normalization;channel normalization;HMD-B51 dataset;UCF101 dataset,,822,,42,,,,,IEEE,IEEE Conferences,CVPR
A Key Volume Mining Deep Framework for Action Recognition, Wangjiang Zhu; Jie Hu; Gang Sun; Xudong Cao; Yu Qiao,NA; NA; NA; NA; NA,2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2016,,,1991,1999,"Recently, deep learning approaches have demonstrated remarkable progresses for action recognition in videos. Most existing deep frameworks equally treat every volume i.e. spatial-temporal video clip, and directly assign a video label to all volumes sampled from it. However, within a video, discriminative actions may occur sparsely in a few key volumes, and most other volumes are irrelevant to the labeled action category. Training with a large proportion of irrelevant volumes will hurt performance. To address this issue, we propose a key volume mining deep framework to identify key volumes and conduct classification simultaneously. Specifically, our framework is trained is optimized in an alternative way integrated to the forward and backward stages of Stochastic Gradient Descent (SGD). In the forward pass, our network mines key volumes for each action class. In the backward pass, it updates network parameters with the help of these mined key volumes. In addition, we propose ""Stochastic out"" to model key volumes from multi-modalities, and an effective yet simple ""unsupervised key volume proposal"" method for high quality volume sampling. Our experiments show that action recognition performance can be significantly improved by mining key volumes, and we achieve state-of-the-art performance on HMDB51 and UCF101 (93.1%).",,,10.1109/CVPR.2016.219,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780588,,Machine learning;Training;Three-dimensional displays;Stochastic processes;Proposals;Neural networks;Computer vision,data mining;gradient methods;image recognition;learning (artificial intelligence);stochastic processes;video signal processing,key volume mining deep framework;action recognition;deep learning;spatial-temporal video clip;video label;stochastic gradient descent;SGD;stochastic out method;unsupervised key volume proposal method;high quality volume sampling,,173,,37,,,,,IEEE,IEEE Conferences,CVPR
Actionness Estimation Using Hybrid Fully Convolutional Networks, Limin Wang; Yu Qiao; Xiaoou Tang; Luc Van Gool,"Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; NA",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2016,,,2708,2717,"Actionness was introduced to quantify the likelihood of containing a generic action instance at a specific location. Accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection. This paper presents a new deep architecture for actionness estimation, called hybrid fully convolutional network (HFCN), which is composed of appearance FCN (A-FCN) and motion FCN (M-FCN). These two FCNs leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion, respectively. In addition, the fully convolutional nature of H-FCN allows it to efficiently process videos with arbitrary sizes. Experiments are conducted on the challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the effectiveness of H-FCN on actionness estimation, which demonstrate that our method achieves superior performance to previous ones. Moreover, we apply the estimated actionness maps on action proposal generation and action detection. Our actionness maps advance the current state-of-the-art performance of these tasks substantially.",,,10.1109/CVPR.2016.296,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780665,,Estimation;Proposals;Visualization;Computer vision;Computer architecture;Feature extraction;Image segmentation,convolution;estimation theory;image motion analysis;neural nets;video signal processing,actionness estimation;hybrid fully convolutional networks;HFCN;generic action instance;video analysis;appearance FCN;motion FCN;static appearance;dynamic motion;Stanford40 datasets;UCF Sports datasets;JHMDB datasets;action proposal generation;action detection;actionness maps,,60,,50,,,,,IEEE,IEEE Conferences,CVPR
Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition, Yandong Wen; Zhifeng Li; Yu Qiao,"Sch. of Electron. & Inf. Eng., South China Univ. of Technol., Guangzhou, China; Shenzhen Key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2016,,,4893,4901,"While considerable progresses have been made on face recognition, age-invariant face recognition (AIFR) still remains a major challenge in real world applications of face recognition systems. The major difficulty of AIFR arises from the fact that the facial appearance is subject to significant intra-personal changes caused by the aging process over time. In order to address this problem, we propose a novel deep face recognition framework to learn the ageinvariant deep face features through a carefully designed CNN model. To the best of our knowledge, this is the first attempt to show the effectiveness of deep CNNs in advancing the state-of-the-art of AIFR. Extensive experiments are conducted on several public domain face aging datasets (MORPH Album2, FGNET, and CACD-VS) to demonstrate the effectiveness of the proposed model over the state-of the-art. We also verify the excellent generalization of our new model on the famous LFW dataset.",,,10.1109/CVPR.2016.529,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780898,,Face recognition;Convolution;Face;Aging;Feature extraction;Robustness;Training,age issues;face recognition;neural nets,latent factor guided convolutional neural networks;age-invariant face recognition;AIFR,,85,,37,,,,,IEEE,IEEE Conferences,CVPR
Real-Time Action Recognition with Enhanced Motion Vector CNNs, Bowen Zhang; Limin Wang; Zhe Wang; Yu Qiao; Hanli Wang,NA; NA; NA; NA; NA,2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2016,,,2718,2726,"The deep two-stream architecture [23] exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.",,,10.1109/CVPR.2016.297,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780666,,Optical imaging;Real-time systems;Streaming media;Noise measurement;Training;Optical computing;Video compression,image recognition;image sequences;learning (artificial intelligence);motion estimation;neural nets;video coding,knowledge learning;recognition performance;video compression;optical flow calculation;video based action recognition;deep two-stream architecture;enhanced motion vector CNNs;real-time action recognition,,217,,37,,,,,IEEE,IEEE Conferences,CVPR
Temporal Hallucinating for Action Recognition with Few Still Images, Yali Wang; Lei Zhou; Yu Qiao,"Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Inst. of Adv. Technol., Shenzhen, China",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,2018,,,5314,5322,"Action recognition in still images has been recently promoted by deep learning. However, the success of these deep models heavily depends on huge amount of training images for various action categories, which may not be available in practice. Alternatively, humans can classify new action categories after seeing few images, since we may not only compare appearance similarities between images on hand, but also attempt to recall importance motion cues from relevant action videos in our memory. To mimic this capacity, we propose a novel Hybrid Video Memory (HVM) machine, which can hallucinate temporal features of still images from video memory, in order to boost action recognition with few still images. First, we design a temporal memory module consisting of temporal hallucinating and predicting. Temporal hallucinating can generate temporal features of still images in an unsupervised manner. Hence, it can be flexibly used in realistic scenarios, where image and video categories may not be consistent. Temporal predicting can effectively infer action categories for query image, by integrating temporal features of training images and videos within a domain-adaptation manner. Second, we design a spatial memory module for spatial predicting. As spatial and temporal features are complementary to represent different actions, we apply spatial-temporal prediction fusion to further boost performance. Finally, we design a video selection module to select strongly-relevant videos as memory. In this case, we can balance the number of images and videos to reduce prediction bias as well as preserve computation efficiency. To show the effectiveness, we conduct extensive experiments on three challenging data sets, where our HVM outperforms a number of recent approaches by temporal hallucinating from video memory.",,,10.1109/CVPR.2018.00557,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578655,,Videos;Training;Image recognition;Memory modules;Optical imaging;Kernel,feature extraction;image classification;image motion analysis;image recognition;image representation;learning (artificial intelligence);video signal processing,action recognition;novel Hybrid Video Memory machine;temporal memory module;video categories;temporal predicting;query image;temporal hallucinating;video selection module;spatial-temporal prediction fusion;temporal features;spatial memory module;training images,,9,,42,,,,,IEEE,IEEE Conferences,CVPR
An End-to-End TextSpotter with Explicit Alignment and Attention, Tong He; Zhi Tian; Weilin Huang; Chunhua Shen; Yu Qiao; Changming Sun,"Univ. of Adelaide, Adelaide, SA, Australia; Univ. of Adelaide, Adelaide, SA, Australia; Malong Technol. Co., Ltd., Shenzhen, China; Univ. of Adelaide, Adelaide, SA, Australia; Shenzhen Inst. of Adv. Technol., Shenzhen, China; Data61, CSIRO, Australia",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,2018,,,5020,5029,"Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Jointly training two tasks is non-trivial due to significant differences in learning difficulties and convergence rates. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in a united framework. Our main contributions are three-fold: (1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; (2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; (3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model obtains impressive results in end-to-end recognition on the ICDAR 2015 [19], significantly advancing the most recent results [2], with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on related benchmarks. Code is available at https://github.com/tonghe90/textspotter.",,,10.1109/CVPR.2018.00527,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578625,,Text recognition;Task analysis;Training;Feature extraction;Image recognition;Convolutional codes;Detectors,character recognition;learning (artificial intelligence);recurrent neural nets;text detection,end-to-end textspotter;text detection;natural images;convolutional features;character attention mechanism;character spatial information;RNN branch;word recognition;end-to-end recognition;text-alignment layer,,46,,43,,,,,IEEE,IEEE Conferences,CVPR
FOTS: Fast Oriented Text Spotting with a Unified Network, Xuebo Liu; Ding Liang; Shi Yan; Dagui Chen; Yu Qiao; Junjie Yan,"NA; NA; NA; NA; Shenzhen Inst. of Adv. Technol., Shenzhen, China; NA",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,2018,,,5676,5685,"Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specifically, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method makes our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps.",,,10.1109/CVPR.2018.00595,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578693,,Text recognition;Feature extraction;Convolution;Proposals;Task analysis;Decoding;Computer vision,character recognition;document image processing;text detection,FOTS;fast oriented text spotting;unified network;incidental scene text spotting;document analysis community;unified end-to-end trainable Fast Oriented Text;convolution sharing strategy;baseline text detection network;joint training method;two-stage methods;ICDAR 2017 MLT;ICDAR 2013 datasets;real-time oriented text spotting system;ICDAR 2015 text spotting task,,93,,54,,,,,IEEE,IEEE Conferences,CVPR
Adaptive Pyramid Context Network for Semantic Segmentation,Junjun He; Zhongying Deng; Lei Zhou; Yali Wang; Yu Qiao,,2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2019,,,,,,,,Adaptive Pyramid Context Network for Semantic Segmentation,,,,,,,,5,,,,,,,,,CVPR
PA3D: Pose-Action 3D Machine for Video Recognition, An Yan; Yali Wang; Zhifeng Li; Yu Qiao,,2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2019,,,,,,,,PA3D Pose-Action 3D Machine for Video Recognition,,,,,,,,3,,,,,,,,,CVPR
MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition, Weihe Zhang; Yali Wang; Yu Qiao,,2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2019,,,,,,,,MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition,,,,,,,,1,,,,,,,,,CVPR
Blind Super-Resolution With Iterative Kernel Correction, Jinjin Gu; Hannan Lu; Wangmeng Zuo; Chao Dong,,2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2019,,,,,,,,Blind Super-Resolution With Iterative Kernel Correction,,,,,,,,9,,,,,,,,,CVPR
Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers, Jingwen He; Chao Dong; Yu Qiao,,2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2019,,,,,,,,Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers,,,,,,,,3,,,,,,,,,CVPR
Deep Network Interpolation for Continuous Imagery Effect Transition, Xintao Wang; Ke Yu; Chao Dong; Xiaoou Tang; Chen Change Loy,,2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2019,,,,,,,,Deep Network Interpolation for Continuous Imagery Effect Transition,,,,,,,,4,,,,,,,,,CVPR
P2SGrad: Refined Gradients for Optimizing Deep Face Models, Xiao Zhang; Rui Zhao; Junjie Yan; Mengya Gao; Yu Qiao; Xiaogang Wang; Hongsheng Li,,2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),,2019,,,,,,,,P2SGrad Refined Gradients for Optimizing Deep Face Models,,,,,,,,2,,,,,,,,,CVPR
Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform, Xintao Wang; Ke Yu; Chao Dong; Chen Change Loy,"NA; NA; Chinese Univ. of Hong Kong, Hong Kong, China; NA",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,2018,,,606,615,"Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27] and EnhanceNet [38].",,,10.1109/CVPR.2018.00070,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578168,,Semantics;Image segmentation;Image resolution;Modulation;Visualization;Training;Transforms,affine transforms;feature extraction;feedforward neural nets;image recognition;image reconstruction;image resolution;image segmentation;image texture;learning (artificial intelligence);probability,high-resolution image;SR network;deep spatial feature;convolutional neural networks;CNN;high-quality reconstruction;single-image super-resolution;semantic classes;semantic segmentation probability maps;transformation parameters;spatial-wise feature modulation;SFT layers;input image;spatial feature transform layer;affine transformation parameters;SRGAN;EnhanceNet;categorical priors,,99,,52,,,,,IEEE,IEEE Conferences,CVPR
Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning, Ke Yu; Chao Dong; Liang Lin; Chen Change Loy,NA; NA; NA; NA,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,2018,,,2443,2452,"We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a stepwise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain1.",,,10.1109/CVPR.2018.00259,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578357,,Image restoration;Tools;Distortion;Task analysis;Transform coding;Complexity theory,image restoration;learning (artificial intelligence),small-scale convolutional networks;RL-Restore;corrupted image;stepwise reward function;human-designed networks;image restoration;deep reinforcement learning,,35,,45,,,,,IEEE,IEEE Conferences,CVPR
Iterative MAP and ML Estimations for Image Segmentation,Shifeng Chen; Liangliang Cao; Jianzhuang Liu; Xiaoou Tang,"Dept. of IE, The Chinese University of Hong Kong. sfchen5@ie.cuhk.edu.hk; Dept. of ECE, University of Illinois at Urbana-Champaign. cao4@uiuc.edu; Dept. of IE, The Chinese University of Hong Kong. jzliu@ie.cuhk.edu.hk; Dept. of IE, The Chinese University of Hong Kong; Microsoft Research Asia, Beijing, China. xitang@microsoft.com",2007 IEEE Conference on Computer Vision and Pattern Recognition,,2007,,,1,6,"Image segmentation plays an important role in computer vision and image analysis. In this paper, the segmentation problem is formulated as a labeling problem under a probability maximization framework. To estimate the label configuration, an iterative optimization scheme is proposed to alternately carry out the maximum a posteriori (MAP) estimation and the maximum-likelihood (ML) estimation. The MAP estimation problem is modeled with Markov random fields (MRFs). A graph-cut algorithm is used to find the solution to the MAP-MRF estimation. The ML estimation is achieved by finding the means of region features. Our algorithm can automatically segment an image into regions with relevant textures or colors without the need to know the number of regions in advance. In addition, under the same framework, it can be extended to another algorithm that extracts objects of a particular class from a group of images. Extensive experiments have shown the effectiveness of our approach.",,,10.1109/CVPR.2007.383007,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270032,,Maximum likelihood estimation;Image segmentation;Computer vision;Iterative algorithms;Clustering algorithms;Asia;Image color analysis;Image texture analysis;Labeling;Markov random fields,computer vision;feature extraction;graph theory;image colour analysis;image segmentation;image texture;iterative methods;Markov processes;maximum likelihood estimation;optimisation;probability;random processes,iterative method;image segmentation;computer vision;labeling problem;probability maximization framework;optimization;maximum a posteriori estimation;maximum-likelihood estimation;Markov random field;graph-cut algorithm;image texture;image color analysis;object extraction,,14,,17,,,,,IEEE,IEEE Conferences,CVPR
Mining Motion Atoms and Phrases for Complex Action Recognition, Limin Wang; Yu Qiao; Xiaoou Tang,"Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China",2013 IEEE International Conference on Computer Vision,,2013,,,2680,2687,"This paper proposes motion atom and phrase as a mid-level temporal ``part'' for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discover a set of representative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representative power. We introduce a bottom-up phrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.",,,10.1109/ICCV.2013.333,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6751444,action recognition;mid-level representation,Motion segmentation;Training;Hidden Markov models;Support vector machines;Image segmentation;Correlation;Equations,data mining;image classification;pattern clustering;video signal processing,UCF50;Olympic sports;complex action dataset;mining task;greedy selection method;bottom-up phrase construction algorithm;effective motion phrase;discriminative clustering method;action video;motion information;complex action classification;midlevel temporal part;complex action recognition;phrases mining;motion atoms mining,,81,,30,,,,,IEEE,IEEE Conferences,ICCV
Range Loss for Deep Face Recognition with Long-Tailed Training Data, Xiao Zhang; Zhiyuan Fang; Yandong Wen; Zhifeng Li; Yu Qiao,"Guangdong Provincial Key Lab. of Comput. Vision & Vitrual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Guangdong Provincial Key Lab. of Comput. Vision & Vitrual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Tencent AI Lab., Shenzhen, China; NA; Guangdong Provincial Key Lab. of Comput. Vision & Vitrual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China",2017 IEEE International Conference on Computer Vision (ICCV),,2017,,,5419,5428,"Deep convolutional neural networks have achieved significant improvements on face recognition task due to their ability to learn highly discriminative features from tremendous amounts of face images. Many large scale face datasets exhibit long-tail distribution where a small number of entities (persons) have large number of face images while a large number of persons only have very few face samples (long tail). Most of the existing works alleviate this problem by simply cutting the tailed data and only keep identities with enough number of examples. Unlike these work, this paper investigated how long-tailed data impact the training of face CNNs and develop a novel loss function, called range loss, to effectively utilize the tailed data in training process. More specifically, range loss is designed to reduce overall intrapersonal variations while enlarge interpersonal differences simultaneously. Extensive experiments on two face recognition benchmarks, Labeled Faces in the Wild (LFW) [11] and YouTube Faces (YTF) [33], demonstrate the effectiveness of the proposed range loss in overcoming the long tail effect, and show the good generalization ability of the proposed methods.",,,10.1109/ICCV.2017.578,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237840,,Face;Training;Face recognition;Data models;Training data;Computer vision,convolution;face recognition;neural nets,Labeled Faces in the Wild;long-tail distribution;face images;highly discriminative features;face recognition task;convolutional neural networks;long-tailed training data;deep face recognition;face recognition benchmarks;face CNNs,,89,,38,,,,,IEEE,IEEE Conferences,ICCV
RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos,Wenbin Du; Yali Wang; Yu Qiao,"Shenzhen Coll. of Adv. Technol., Univ. of Chinese Acad. of Sci., Shenzhen, China; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China",2017 IEEE International Conference on Computer Vision (ICCV),,2017,,,3745,3754,"Recent studies demonstrate the effectiveness of Recurrent Neural Networks (RNNs) for action recognition in videos. However, previous works mainly utilize video-level category as supervision to train RNNs, which may prohibit RNNs to learn complex motion structures along time. In this paper, we propose a recurrent pose-attention network (RPAN) to address this challenge, where we introduce a novel pose-attention mechanism to adaptively learn pose-related features at every time-step action prediction of RNNs. More specifically, we make three main contributions in this paper. Firstly, unlike previous works on pose-related action recognition, our RPAN is an end-to-end recurrent network which can exploit important spatial-temporal evolutions of human pose to assist action recognition in a unified framework. Secondly, instead of learning individual human-joint features separately, our pose-attention mechanism learns robust human-part features by sharing attention parameters partially on the semantically-related human joints. These human-part features are then fed into the human-part pooling layer to construct a highly-discriminative pose-related representation for temporal action modeling. Thirdly, one important byproduct of our RPAN is pose estimation in videos, which can be used for coarse pose annotation in action videos. We evaluate the proposed RPAN quantitatively and qualitatively on two popular benchmarks, i.e., Sub-JHMDB and PennAction. Experimental results show that RPAN outperforms the recent state-of-the-art methods on these challenging datasets.",,,10.1109/ICCV.2017.402,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237664,,Videos;Robustness;Heating systems;Computer vision;Recurrent neural networks;Pose estimation;Image recognition,feature extraction;image motion analysis;pose estimation;recurrent neural nets;video signal processing,RPAN;Recurrent Neural Networks;RNNs;video-level category;complex motion structures;pose-attention mechanism;time-step action prediction;pose-related action recognition;human-joint features;robust human-part features;attention parameters;human-part pooling layer;temporal action modeling;action videos;recurrent pose-attention network,,55,,54,,,,,IEEE,IEEE Conferences,ICCV
Single Shot Text Detector with Regional Attention, Pan He; Weilin Huang; Tong He; Qile Zhu; Yu Qiao; Xiaolin Li,"Nat. Sci. Found. Center for Big Learning, Univ. of Florida, Gainesville, FL, USA; Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Nat. Sci. Found. Center for Big Learning, Univ. of Florida, Gainesville, FL, USA; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Nat. Sci. Found. Center for Big Learning, Univ. of Florida, Gainesville, FL, USA",2017 IEEE International Conference on Computer Vision (ICCV),,2017,,,3066,3074,"We present a novel single-shot text detector that directly outputs word-level bounding boxes in a natural image. We propose an attention mechanism which roughly identifies text regions via an automatically learned attentional map. This substantially suppresses background interference in the convolutional features, which is the key to producing accurate inference of words, particularly at extremely small sizes. This results in a single model that essentially works in a coarse-to-fine manner. It departs from recent FCN-based text detectors which cascade multiple FCN models to achieve an accurate prediction. Furthermore, we develop a hierarchical inception module which efficiently aggregates multi-scale inception features. This enhances local details, and also encodes strong context information, allowing the detector to work reliably on multi-scale and multi-orientation text with single-scale images. Our text detector achieves an F-measure of 77% on the ICDAR 2015 benchmark, advancing the state-of-the-art results in [18, 28]. Demo is available at: http://sstd.whuang.org/.",,,10.1109/ICCV.2017.331,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237593,,Detectors;Convolutional codes;Predictive models;Heating systems;Computer vision;Interference,convolution;feature extraction;natural scenes;neural nets;text detection,single shot text detector;regional attention;word-level bounding boxes;natural image;attention mechanism;text regions;background interference;convolutional features;hierarchical inception module;single-scale images;attentional map;FCN-based text detectors;multiscale inception features;Fully Convolutional Network,,115,,38,,,,,IEEE,IEEE Conferences,ICCV
Detecting Faces Using Inside Cascaded Contextual CNN, Kaipeng Zhang; Zhanpeng Zhang; Hao Wang; Zhifeng Li; Yu Qiao; Wei Liu,"NA; NA; NA; NA; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; NA",2017 IEEE International Conference on Computer Vision (ICCV),,2017,,,3190,3198,"Deep Convolutional Neural Networks (CNNs) achieve substantial improvements in face detection in the wild. Classical CNN-based face detection methods simply stack successive layers of filters where an input sample should pass through all layers before reaching a face/non-face decision. Inspired by the fact that for face detection, filters in deeper layers can discriminate between difficult face/non-face samples while those in shallower layers can efficiently reject simple non-face samples, we propose Inside Cascaded Structure that introduces face/non-face classifiers at different layers within the same CNN. In the training phase, we propose data routing mechanism which enables different layers to be trained by different types of samples, and thus deeper layers can focus on handling more difficult samples compared with traditional architecture. In addition, we introduce a two-stream contextual CNN architecture that leverages body part information adaptively to enhance face detection. Extensive experiments on the challenging FD-DB and WIDER FACE benchmarks demonstrate that our method achieves competitive accuracy to the state-of-the-art techniques while keeps real time performance.",,,10.1109/ICCV.2017.344,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237606,,Face detection;Training;Integrated circuits;Feature extraction;Routing;Computer architecture;Testing,convolution;face recognition;image classification;image enhancement;learning (artificial intelligence);neural nets,detection methods;deeper layers;shallower layers;face samples;face decision;nonface decision;inside cascaded contextual CNN;nonface samples;face classifiers;nonface classifiers;deep convolutional neural networks;face detection enhancement;WIDER FACE benchmarks;two-stream contextual CNN architecture,,25,,32,,,,,IEEE,IEEE Conferences,ICCV
RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution,Wenlong Zhang; Yihao Liu; Chao Dong; Yu Qiao,,2019 IEEE International Conference on Computer Vision (ICCV),,2019,,,,,,,,RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution,,,,,,,,3,,,,,,,,,ICCV
LAP-Net: Level-Aware Progressive Network for Image Dehazing,Yunan Li; Qiguang Miao; Wanli Ouyang; Zhenxin Ma; Huijuan Fang; Chao Dong; Yining Quan,,2019 IEEE International Conference on Computer Vision (ICCV),,2019,,,,,,,,LAP-Net Level-Aware Progressive Network for Image Dehazing,,,,,,,,0,,,,,,,,,ICCV
DF2Net: A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction,Xiaoxing Zeng; Xiaojiang Peng; Yu Qiao,,2019 IEEE International Conference on Computer Vision (ICCV),,2019,,,,,,,,DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction,,,,,,,,0,,,,,,,,,ICCV
Super-Identity Convolutional Neural Network for Face Hallucination,Kaipeng Zhang;Zhanpeng Zhang;Chia-Wen Cheng;Winston H. Hsu;Yu Qiao;Wei Liu;Tong Zhang,,,,2018,,,,,,,,10.1007/978-3-030-01252-6,,,Face hallucination;Super identity;Domain-integrated training;Convolutional neural networks ,,,,,18,,,,,,,,,ECCV
Find and Focus: Retrieve and Localize Video Events with Natural Language Queries,Dian Shao;Yu Xiong;Yue Zhao;Qingqiu Huang;Yu Qiao;Dahua Lin,,,,2018,,,,,,,,10.1007/978-3-030-01240-3,,,,,,,,7,,,,,,,,,ECCV
Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,Limin Wang;Yuanjun Xiong;Zhe Wang;Yu Qiao;Dahua Lin;Xiaoou Tang;Luc Van Gool,,,,2016,,,,,,,,10.1007/978-3-319-46484-8_2,,,Action recognition; Temporal segment networks; Good practices; ConvNets ,,,,,1168,,,,,,,,,ECCV
Detecting Text in Natural Image with Connectionist Text Proposal Network, Zhi Tian; Weilin Huang; Tong He; Pan He; Yu Qiao;,,,,2016,,,,,,,,10.1007/978-3-319-46484-8_4,,,Scene text detection; Convolutional network; Recurrent neural network; Anchor mechanism ,,,,,326,,,,,,,,,ECCV
A Discriminative Feature Learning Approach for Deep Face Recognition,Yandong Wen; Kaipeng Zhang; Zhifeng Li; Yu Qiao,,,,2016,,,,,,,,10.1007/978-3-319-46478-7,,,Convolutional neural networks; Face recognition; Discriminative feature learning; Center loss ,,,,,1189,,,,,,,,,ECCV
Robust Scene Text Detection with Convolution Neural Network Induced MSER Trees, Weilin Huang; Yu Qiao; Xiaoou Tang,,,,2014,,,,,,,,10.1007/978-3-319-10593-2,,,Maximally Stable Extremal Regions (MSERs); convolutional neural network (CNN); text-like outliers; sliding-window ,,,,,284,,,,,,,,,ECCV
Video Action Detection with Relational Dynamic-Poselets, Limin Wang; Yu Qiao; Xiaoou Tang,,,,2014,,,,,"Action detection is of great importance in understanding human motion from video. Compared with action recognition, it not only recognizes action type, but also localizes its spatiotemporal extent. This paper presents a relational model for action detection, which first decomposes human action into temporal ¡°key poses¡± and then further into spatial ¡°action parts¡±. Specifically, we start by clustering cuboids around each human joint into dynamic-poselets using a new descriptor. The cuboids from the same cluster share consistent geometric and dynamic structure, and each cluster acts as a mixture of body parts. We then propose a sequential skeleton model to capture the relations among dynamic-poselets. This model unifies the tasks of learning the composites of mixture dynamic-poselets, the spatiotemporal structures of action parts, and the local model for each action part in a single framework. Our model not only allows to localize the action in a video stream, but also enables a detailed pose estimation of an actor. We formulate the model learning problem in a structured SVM framework and speed up model inference by dynamic programming. We conduct experiments on three challenging action detection datasets: the MSR-II dataset, the UCF Sports dataset, and the JHMDB dataset. The results show that our method achieves superior performance to the state-of-the-art methods on these datasets.",,,10.1007/978-3-319-10602-1,,,Action detection; dynamic-poselet; sequential skeleton model,,,,,105,,,,,,,,,ECCV
Action Recognition with Stacked Fisher Vectors,Xiaojiang Peng;Changqing Zou;Yu Qiao;Qiang Peng,,,,2014,,,,,,,,10.1007/978-3-319-10602-1_38,,,Action recognition; Fisher vectors; stacked Fisher vectors; max-margin dimensionality reduction ,,,,,312,,,,,,,,,ECCV
Boosting VLAD with Supervised Dictionary Learning and High-Order Statistics,Xiaojiang Peng; Limin Wang; Yu Qiao; Qiang Peng,,,,2014,,,,,,,,10.1007/978-3-319-10578-9,,,Visual Word; Action Recognition; Local Descriptor; Sparse Code; Convolutional Neural Network,,,,,68,,,,,,,,,ECCV
SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters,Yifan Xu;Tianqi Fan;Mingye Xu;Long Zeng;Yu Qiao,,,,2018,,,,,,,,10.1007/978-3-030-01237-3_6,,,Convolutional neural network; Parametrized convolutional filters; Point clouds ,,,,,80,,,,,,,,,ECCV
Learning a Deep Convolutional Network for Image Super-Resolution,Chao Dong;Chen Change Loy;Kaiming He;Xiaoou Tang,,,,2014,,,,,,,,10.1007/978-3-319-10593-2_13,,,Super-resolution; deep convolutional neural networks ,,,,,1940,,,,,,,,,ECCV
Accelerating the super-resolution convolutional neural network,Chao Dong;Chen Change Loy;Xiaoou Tang,,,,2016,,,,,,,,10.1007/978-3-319-46475-6_25,,,Mapping Layer; Deep Model; Interpolation Kernel; Convolution Filter; Convolution Layer ,,,,,734,,,,,,,,,ECCV
Esrgan: Enhanced super-resolution generative adversarial networks,Xintao Wang;Ke Yu;Shixiang Wu;Jinjin Gu;Yihao Liu;Chao Dong;Yu Qiao;Chen Change Loy,,,,2018,,,,,,,,10.1007/978-3-030-11021-5_5,,,,,,,,199,,,,,,,,,ECCV
Multi-region two-stream R-CNN for action detection,Xiaojiang Peng;Cordelia Schmid,,,,2016,,,,,,,,10.1007/978-3-319-46493-0_45,,,Action detection; Faster R-CNN; Multi-region CNNs; Two stream R-CNN ,,,,,186,,,,,,,,,ECCV
