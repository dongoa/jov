var NC2P={"More than 2":[],"10-20":["3","5","9","10","11","12","13","14","16","17","18","19","22","23","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42"],"5-10":["0","1","2","4","6","7","8","15","20","21","24","25"],"Less than 5":[]};
var NC2P2={"More than 1000":["3","31","33","39","40"],"50-100":["0","5","6","10","18","21","22","23","37","38"],"100-1000":["1","2","4","7","24","32","34","35","36","41","42"],"Less than 50":["8","9","11","12","13","14","15","16","17","19","20","25","26","27","28","29","30"]};
var V2P = {"CVPR":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],"ECCV":[29,30,31,32,33,34,35,36,37,38,39,40,41,42],"ICCV":[21,22,23,24,25,26,27,28]};
var A2P={"Yu Qiao":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15,17,21,22,23,24,25,26,28,29,30,31,32,33,34,35,36,37,38,41],"Jianzhuang Liu":[0,20],"Xiaoou Tang":[0,1,3,5,16,20,21,31,34,35,39,40],"Limin Wang":[1,2,3,5,7,21,31,35,37],"Zhuowei Cai":[2],"Xiaojiang Peng":[2,28,36,37,42],"Wangjiang Zhu":[4],"Jie Hu":[4],"Gang Sun":[4],"Xudong Cao":[4],"Luc Van Gool":[5,31],"Yandong Wen":[6,22,33],"Zhifeng Li":[6,12,22,25,33],"Bowen Zhang":[7],"Zhe Wang":[7,31],"Hanli Wang":[7],"Yali Wang":[8,11,12,13,23],"Lei Zhou":[8,11],"Tong He":[9,24,32],"Zhi Tian":[9,32],"Weilin Huang":[9,24,32,34],"Chunhua Shen":[9],"Changming Sun":[9],"Xuebo Liu":[10],"Ding Liang":[10],"Shi Yan":[10],"Dagui Chen":[10],"Junjie Yan":[10,17],"Junjun He":[11],"Zhongying Deng":[11],"An Yan":[12],"Weihe Zhang":[13],"Jinjin Gu":[14,41],"Hannan Lu":[14],"Wangmeng Zuo":[14],"Chao Dong":[14,15,16,18,19,26,27,39,40,41],"Jingwen He":[15],"Xintao Wang":[16,18,41],"Ke Yu":[16,18,19,41],"Chen Change Loy":[16,18,19,39,40,41],"Xiao Zhang":[17,22],"Rui Zhao":[17],"Mengya Gao":[17],"Xiaogang Wang":[17],"Hongsheng Li":[17],"Liang Lin":[19],"Shifeng Chen":[20],"Liangliang Cao":[20],"Zhiyuan Fang":[22],"Wenbin Du":[23],"Pan He":[24,32],"Qile Zhu":[24],"Xiaolin Li":[24],"Kaipeng Zhang":[25,29,33],"Zhanpeng Zhang":[25,29],"Hao Wang":[25],"Wei Liu":[25,29],"Wenlong Zhang":[26],"Yihao Liu":[26,41],"Yunan Li":[27],"Qiguang Miao":[27],"Wanli Ouyang":[27],"Zhenxin Ma":[27],"Huijuan Fang":[27],"Yining Quan":[27],"Xiaoxing Zeng":[28],"Chia-Wen Cheng":[29],"Winston H. Hsu":[29],"Tong Zhang":[29],"Dian Shao":[30],"Yu Xiong":[30],"Yue Zhao":[30],"Qingqiu Huang":[30],"Dahua Lin":[30,31],"Yuanjun Xiong":[31],"":[32],"Changqing Zou":[36],"Qiang Peng":[36,37],"Yifan Xu":[38],"Tianqi Fan":[38],"Mingye Xu":[38],"Long Zeng":[38],"Kaiming He":[39],"Shixiang Wu":[41],"Cordelia Schmid":[42]};
var PAPER=[{"Document Title":"Offline Signature Verification Using Online Handwriting Registration","Authors":" Yu Qiao; Jianzhuang Liu; Xiaoou Tang","Author Affiliations":"Department of Information Engineering, The Chinese University of Hong Kong, qiao@gavo.t.u-tokyo.ac.jp; Department of Information Engineering, The Chinese University of Hong Kong, jzliu@ie.cuhk.edu.hk; Microsoft Research Asia, Beijing, China. xitang@microsoft.com","Publication Title":"2007 IEEE Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2007","Volume":"","Issue":"","Start Page":"1","End Page":"8","Abstract":"This paper proposes a novel framework for offline signature verification. Different from previous methods, our approach makes use of online handwriting instead of handwritten images for registration. The online registrations enable robust recovery of the writing trajectory from an input offline signature and thus allow effective shape matching between registration and verification signatures. In addition, we propose several new techniques to improve the performance of the new signature verification system: 1. we formulate and solve the recovery of writing trajectory within the framework of conditional random fields; 2. we propose a new shape descriptor, online context, for aligning signatures; 3. we develop a verification criterion which combines the duration and amplitude variances of handwriting. Experiments on a benchmark database show that the proposed method significantly outperforms the well-known offline signature verification methods and achieve comparable performance with online signature verification methods.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2007.383263","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270288","Author Keywords":"","IEEE Terms":"Handwriting recognition;Writing;Shape;Feature extraction;Data mining;Asia;Robustness;Databases;Art;Error analysis","INSPEC Controlled Terms":"handwriting recognition;image matching;image registration","INSPEC Non-Controlled Terms":"offline signature verification;online handwriting registration;handwritten images;shape matching","Mesh_Terms":"","Article Citation Count":"66","Patent Citation Count":"","Reference Count":"26","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Motionlets: Mid-level 3D Parts for Human Motion Recognition","Authors":" Limin Wang; Yu Qiao; Xiaoou Tang","Author Affiliations":"Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","Publication Title":"2013 IEEE Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2013","Volume":"","Issue":"","Start Page":"2674","End Page":"2681","Abstract":"This paper proposes motionlet, a mid-level and spatiotemporal part, for human motion recognition. Motion let can be seen as a tight cluster in motion and appearance space, corresponding to the moving process of different body parts. We postulate three key properties of motion let for action recognition: high motion saliency, multiple scale representation, and representative-discriminative ability. Towards this goal, we develop a data-driven approach to learn motion lets from training videos. First, we extract 3D regions with high motion saliency. Then we cluster these regions and preserve the centers as candidate templates for motion let. Finally, we examine the representative and discriminative power of the candidates, and introduce a greedy method to select effective candidates. With motion lets, we present a mid-level representation for video, called motionlet activation vector. We conduct experiments on three datasets, KTH, HMDB51, and UCF50. The results show that the proposed methods significantly outperform state-of-the-art methods.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2013.345","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619189","Author Keywords":"","IEEE Terms":"Videos;Three-dimensional displays;Feature extraction;Spatiotemporal phenomena;Histograms;Vectors;Detectors","INSPEC Controlled Terms":"computer graphics;feature extraction;image motion analysis;video signal processing","INSPEC Non-Controlled Terms":"mid-level 3D parts;human motion recognition;spatiotemporal part;body parts;action recognition;high motion saliency;multiple scale representation;representative-discriminative ability;data-driven approach;training videos;3D region extraction;motionlet activation vector;KTH dataset;HMDB51 dataset;UCF50 dataset","Mesh_Terms":"","Article Citation Count":"187","Patent Citation Count":"","Reference Count":"32","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Multi-view Super Vector for Action Recognition","Authors":" Zhuowei Cai; Limin Wang; Xiaojiang Peng; Yu Qiao","Author Affiliations":"Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comput. Vision & Pattern Recognition, Shenzhen Inst. of Adv. Technol., Shenzhen, China","Publication Title":"2014 IEEE Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2014","Volume":"","Issue":"","Start Page":"596","End Page":"603","Abstract":"Images and videos are often characterized by multiple types of local descriptors such as SIFT, HOG and HOF, each of which describes certain aspects of object feature. Recognition systems benefit from fusing multiple types of these descriptors. Two widely applied fusion pipelines are descriptor concatenation and kernel average. The first one is effective when different descriptors are strongly correlated, while the second one is probably better when descriptors are relatively independent. In practice, however, different descriptors are neither fully independent nor fully correlated, and previous fusion methods may not be satisfying. In this paper, we propose a new global representation, Multi-View Super Vector (MVSV), which is composed of relatively independent components derived from a pair of descriptors. Kernel average is then applied on these components to produce recognition result. To obtain MVSV, we develop a generative mixture model of probabilistic canonical correlation analyzers (M-PCCA), and utilize the hidden factors and gradient vectors of M-PCCA to construct MVSV for video representation. Experiments on video based action recognition tasks show that MVSV achieves promising results, and outperforms FV and VLAD with descriptor concatenation or kernel average fusion strategy.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2014.83","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909477","Author Keywords":"multi-view;action recognition;mixture model;canonical correlation analysis","IEEE Terms":"Vectors;Videos;Kernel;Correlation;Probabilistic logic;Accuracy;Encoding","INSPEC Controlled Terms":"feature extraction;image motion analysis;image representation;object recognition;statistical analysis;video signal processing","INSPEC Non-Controlled Terms":"multiview super vector;SIFT descriptors;HOG descriptors;HOF descriptors;scale invariant feature transforms;histogram-of-oriented gradients;object feature;fusion pipelines;descriptor concatenation;kernel average;MVSV representation;M-PCCA;generative mixture model of probabilistic canonical correlation analyzers;video based action recognition tasks","Mesh_Terms":"","Article Citation Count":"154","Patent Citation Count":"","Reference Count":"39","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Action recognition with trajectory-pooled deep-convolutional descriptors","Authors":"Limin Wang; Yu Qiao; Xiaoou Tang","Author Affiliations":"Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong","Publication Title":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2015","Volume":"","Issue":"","Start Page":"4305","End Page":"4314","Abstract":"Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2015.7299059","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299059","Author Keywords":"","IEEE Terms":"Trajectory;Feature extraction;Videos;Optical imaging;Spatiotemporal phenomena;Visualization;Three-dimensional displays","INSPEC Controlled Terms":"convolution;learning (artificial intelligence);video signal processing","INSPEC Non-Controlled Terms":"action recognition;trajectory-pooled deep-convolutional descriptors;TDD;visual features;human action understanding;video representation;hand-crafted features;deep-learned features;deep architectures;discriminative convolutional feature maps;spatiotemporal normalization;channel normalization;HMD-B51 dataset;UCF101 dataset","Mesh_Terms":"","Article Citation Count":"822","Patent Citation Count":"","Reference Count":"42","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"A Key Volume Mining Deep Framework for Action Recognition","Authors":" Wangjiang Zhu; Jie Hu; Gang Sun; Xudong Cao; Yu Qiao","Author Affiliations":"NA; NA; NA; NA; NA","Publication Title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"1991","End Page":"1999","Abstract":"Recently, deep learning approaches have demonstrated remarkable progresses for action recognition in videos. Most existing deep frameworks equally treat every volume i.e. spatial-temporal video clip, and directly assign a video label to all volumes sampled from it. However, within a video, discriminative actions may occur sparsely in a few key volumes, and most other volumes are irrelevant to the labeled action category. Training with a large proportion of irrelevant volumes will hurt performance. To address this issue, we propose a key volume mining deep framework to identify key volumes and conduct classification simultaneously. Specifically, our framework is trained is optimized in an alternative way integrated to the forward and backward stages of Stochastic Gradient Descent (SGD). In the forward pass, our network mines key volumes for each action class. In the backward pass, it updates network parameters with the help of these mined key volumes. In addition, we propose \"Stochastic out\" to model key volumes from multi-modalities, and an effective yet simple \"unsupervised key volume proposal\" method for high quality volume sampling. Our experiments show that action recognition performance can be significantly improved by mining key volumes, and we achieve state-of-the-art performance on HMDB51 and UCF101 (93.1%).","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2016.219","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780588","Author Keywords":"","IEEE Terms":"Machine learning;Training;Three-dimensional displays;Stochastic processes;Proposals;Neural networks;Computer vision","INSPEC Controlled Terms":"data mining;gradient methods;image recognition;learning (artificial intelligence);stochastic processes;video signal processing","INSPEC Non-Controlled Terms":"key volume mining deep framework;action recognition;deep learning;spatial-temporal video clip;video label;stochastic gradient descent;SGD;stochastic out method;unsupervised key volume proposal method;high quality volume sampling","Mesh_Terms":"","Article Citation Count":"173","Patent Citation Count":"","Reference Count":"37","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Actionness Estimation Using Hybrid Fully Convolutional Networks","Authors":" Limin Wang; Yu Qiao; Xiaoou Tang; Luc Van Gool","Author Affiliations":"Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; NA","Publication Title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"2708","End Page":"2717","Abstract":"Actionness was introduced to quantify the likelihood of containing a generic action instance at a specific location. Accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection. This paper presents a new deep architecture for actionness estimation, called hybrid fully convolutional network (HFCN), which is composed of appearance FCN (A-FCN) and motion FCN (M-FCN). These two FCNs leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion, respectively. In addition, the fully convolutional nature of H-FCN allows it to efficiently process videos with arbitrary sizes. Experiments are conducted on the challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the effectiveness of H-FCN on actionness estimation, which demonstrate that our method achieves superior performance to previous ones. Moreover, we apply the estimated actionness maps on action proposal generation and action detection. Our actionness maps advance the current state-of-the-art performance of these tasks substantially.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2016.296","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780665","Author Keywords":"","IEEE Terms":"Estimation;Proposals;Visualization;Computer vision;Computer architecture;Feature extraction;Image segmentation","INSPEC Controlled Terms":"convolution;estimation theory;image motion analysis;neural nets;video signal processing","INSPEC Non-Controlled Terms":"actionness estimation;hybrid fully convolutional networks;HFCN;generic action instance;video analysis;appearance FCN;motion FCN;static appearance;dynamic motion;Stanford40 datasets;UCF Sports datasets;JHMDB datasets;action proposal generation;action detection;actionness maps","Mesh_Terms":"","Article Citation Count":"60","Patent Citation Count":"","Reference Count":"50","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition","Authors":" Yandong Wen; Zhifeng Li; Yu Qiao","Author Affiliations":"Sch. of Electron. & Inf. Eng., South China Univ. of Technol., Guangzhou, China; Shenzhen Key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China","Publication Title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"4893","End Page":"4901","Abstract":"While considerable progresses have been made on face recognition, age-invariant face recognition (AIFR) still remains a major challenge in real world applications of face recognition systems. The major difficulty of AIFR arises from the fact that the facial appearance is subject to significant intra-personal changes caused by the aging process over time. In order to address this problem, we propose a novel deep face recognition framework to learn the ageinvariant deep face features through a carefully designed CNN model. To the best of our knowledge, this is the first attempt to show the effectiveness of deep CNNs in advancing the state-of-the-art of AIFR. Extensive experiments are conducted on several public domain face aging datasets (MORPH Album2, FGNET, and CACD-VS) to demonstrate the effectiveness of the proposed model over the state-of the-art. We also verify the excellent generalization of our new model on the famous LFW dataset.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2016.529","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780898","Author Keywords":"","IEEE Terms":"Face recognition;Convolution;Face;Aging;Feature extraction;Robustness;Training","INSPEC Controlled Terms":"age issues;face recognition;neural nets","INSPEC Non-Controlled Terms":"latent factor guided convolutional neural networks;age-invariant face recognition;AIFR","Mesh_Terms":"","Article Citation Count":"85","Patent Citation Count":"","Reference Count":"37","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Real-Time Action Recognition with Enhanced Motion Vector CNNs","Authors":" Bowen Zhang; Limin Wang; Zhe Wang; Yu Qiao; Hanli Wang","Author Affiliations":"NA; NA; NA; NA; NA","Publication Title":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"2718","End Page":"2726","Abstract":"The deep two-stream architecture [23] exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2016.297","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780666","Author Keywords":"","IEEE Terms":"Optical imaging;Real-time systems;Streaming media;Noise measurement;Training;Optical computing;Video compression","INSPEC Controlled Terms":"image recognition;image sequences;learning (artificial intelligence);motion estimation;neural nets;video coding","INSPEC Non-Controlled Terms":"knowledge learning;recognition performance;video compression;optical flow calculation;video based action recognition;deep two-stream architecture;enhanced motion vector CNNs;real-time action recognition","Mesh_Terms":"","Article Citation Count":"217","Patent Citation Count":"","Reference Count":"37","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Temporal Hallucinating for Action Recognition with Few Still Images","Authors":" Yali Wang; Lei Zhou; Yu Qiao","Author Affiliations":"Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Inst. of Adv. Technol., Shenzhen, China","Publication Title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"5314","End Page":"5322","Abstract":"Action recognition in still images has been recently promoted by deep learning. However, the success of these deep models heavily depends on huge amount of training images for various action categories, which may not be available in practice. Alternatively, humans can classify new action categories after seeing few images, since we may not only compare appearance similarities between images on hand, but also attempt to recall importance motion cues from relevant action videos in our memory. To mimic this capacity, we propose a novel Hybrid Video Memory (HVM) machine, which can hallucinate temporal features of still images from video memory, in order to boost action recognition with few still images. First, we design a temporal memory module consisting of temporal hallucinating and predicting. Temporal hallucinating can generate temporal features of still images in an unsupervised manner. Hence, it can be flexibly used in realistic scenarios, where image and video categories may not be consistent. Temporal predicting can effectively infer action categories for query image, by integrating temporal features of training images and videos within a domain-adaptation manner. Second, we design a spatial memory module for spatial predicting. As spatial and temporal features are complementary to represent different actions, we apply spatial-temporal prediction fusion to further boost performance. Finally, we design a video selection module to select strongly-relevant videos as memory. In this case, we can balance the number of images and videos to reduce prediction bias as well as preserve computation efficiency. To show the effectiveness, we conduct extensive experiments on three challenging data sets, where our HVM outperforms a number of recent approaches by temporal hallucinating from video memory.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2018.00557","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578655","Author Keywords":"","IEEE Terms":"Videos;Training;Image recognition;Memory modules;Optical imaging;Kernel","INSPEC Controlled Terms":"feature extraction;image classification;image motion analysis;image recognition;image representation;learning (artificial intelligence);video signal processing","INSPEC Non-Controlled Terms":"action recognition;novel Hybrid Video Memory machine;temporal memory module;video categories;temporal predicting;query image;temporal hallucinating;video selection module;spatial-temporal prediction fusion;temporal features;spatial memory module;training images","Mesh_Terms":"","Article Citation Count":"9","Patent Citation Count":"","Reference Count":"42","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"An End-to-End TextSpotter with Explicit Alignment and Attention","Authors":" Tong He; Zhi Tian; Weilin Huang; Chunhua Shen; Yu Qiao; Changming Sun","Author Affiliations":"Univ. of Adelaide, Adelaide, SA, Australia; Univ. of Adelaide, Adelaide, SA, Australia; Malong Technol. Co., Ltd., Shenzhen, China; Univ. of Adelaide, Adelaide, SA, Australia; Shenzhen Inst. of Adv. Technol., Shenzhen, China; Data61, CSIRO, Australia","Publication Title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"5020","End Page":"5029","Abstract":"Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Jointly training two tasks is non-trivial due to significant differences in learning difficulties and convergence rates. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in a united framework. Our main contributions are three-fold: (1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; (2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; (3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model obtains impressive results in end-to-end recognition on the ICDAR 2015 [19], significantly advancing the most recent results [2], with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on related benchmarks. Code is available at https://github.com/tonghe90/textspotter.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2018.00527","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578625","Author Keywords":"","IEEE Terms":"Text recognition;Task analysis;Training;Feature extraction;Image recognition;Convolutional codes;Detectors","INSPEC Controlled Terms":"character recognition;learning (artificial intelligence);recurrent neural nets;text detection","INSPEC Non-Controlled Terms":"end-to-end textspotter;text detection;natural images;convolutional features;character attention mechanism;character spatial information;RNN branch;word recognition;end-to-end recognition;text-alignment layer","Mesh_Terms":"","Article Citation Count":"46","Patent Citation Count":"","Reference Count":"43","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"FOTS: Fast Oriented Text Spotting with a Unified Network","Authors":" Xuebo Liu; Ding Liang; Shi Yan; Dagui Chen; Yu Qiao; Junjie Yan","Author Affiliations":"NA; NA; NA; NA; Shenzhen Inst. of Adv. Technol., Shenzhen, China; NA","Publication Title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"5676","End Page":"5685","Abstract":"Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specifically, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method makes our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2018.00595","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578693","Author Keywords":"","IEEE Terms":"Text recognition;Feature extraction;Convolution;Proposals;Task analysis;Decoding;Computer vision","INSPEC Controlled Terms":"character recognition;document image processing;text detection","INSPEC Non-Controlled Terms":"FOTS;fast oriented text spotting;unified network;incidental scene text spotting;document analysis community;unified end-to-end trainable Fast Oriented Text;convolution sharing strategy;baseline text detection network;joint training method;two-stage methods;ICDAR 2017 MLT;ICDAR 2013 datasets;real-time oriented text spotting system;ICDAR 2015 text spotting task","Mesh_Terms":"","Article Citation Count":"93","Patent Citation Count":"","Reference Count":"54","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Adaptive Pyramid Context Network for Semantic Segmentation","Authors":"Junjun He; Zhongying Deng; Lei Zhou; Yali Wang; Yu Qiao","Author Affiliations":"","Publication Title":"2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"Adaptive Pyramid Context Network for Semantic Segmentation","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"5","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"CVPR"},{"Document Title":"PA3D: Pose-Action 3D Machine for Video Recognition","Authors":" An Yan; Yali Wang; Zhifeng Li; Yu Qiao","Author Affiliations":"","Publication Title":"2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"PA3D Pose-Action 3D Machine for Video Recognition","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"3","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"CVPR"},{"Document Title":"MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition","Authors":" Weihe Zhang; Yali Wang; Yu Qiao","Author Affiliations":"","Publication Title":"2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"1","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"CVPR"},{"Document Title":"Blind Super-Resolution With Iterative Kernel Correction","Authors":" Jinjin Gu; Hannan Lu; Wangmeng Zuo; Chao Dong","Author Affiliations":"","Publication Title":"2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"Blind Super-Resolution With Iterative Kernel Correction","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"9","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"CVPR"},{"Document Title":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers","Authors":" Jingwen He; Chao Dong; Yu Qiao","Author Affiliations":"","Publication Title":"2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"3","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"CVPR"},{"Document Title":"Deep Network Interpolation for Continuous Imagery Effect Transition","Authors":" Xintao Wang; Ke Yu; Chao Dong; Xiaoou Tang; Chen Change Loy","Author Affiliations":"","Publication Title":"2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"Deep Network Interpolation for Continuous Imagery Effect Transition","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"4","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"CVPR"},{"Document Title":"P2SGrad: Refined Gradients for Optimizing Deep Face Models","Authors":" Xiao Zhang; Rui Zhao; Junjie Yan; Mengya Gao; Yu Qiao; Xiaogang Wang; Hongsheng Li","Author Affiliations":"","Publication Title":"2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"P2SGrad Refined Gradients for Optimizing Deep Face Models","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"2","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"CVPR"},{"Document Title":"Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform","Authors":" Xintao Wang; Ke Yu; Chao Dong; Chen Change Loy","Author Affiliations":"NA; NA; Chinese Univ. of Hong Kong, Hong Kong, China; NA","Publication Title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"606","End Page":"615","Abstract":"Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27] and EnhanceNet [38].","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2018.00070","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578168","Author Keywords":"","IEEE Terms":"Semantics;Image segmentation;Image resolution;Modulation;Visualization;Training;Transforms","INSPEC Controlled Terms":"affine transforms;feature extraction;feedforward neural nets;image recognition;image reconstruction;image resolution;image segmentation;image texture;learning (artificial intelligence);probability","INSPEC Non-Controlled Terms":"high-resolution image;SR network;deep spatial feature;convolutional neural networks;CNN;high-quality reconstruction;single-image super-resolution;semantic classes;semantic segmentation probability maps;transformation parameters;spatial-wise feature modulation;SFT layers;input image;spatial feature transform layer;affine transformation parameters;SRGAN;EnhanceNet;categorical priors","Mesh_Terms":"","Article Citation Count":"99","Patent Citation Count":"","Reference Count":"52","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning","Authors":" Ke Yu; Chao Dong; Liang Lin; Chen Change Loy","Author Affiliations":"NA; NA; NA; NA","Publication Title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"2443","End Page":"2452","Abstract":"We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a stepwise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain1.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2018.00259","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578357","Author Keywords":"","IEEE Terms":"Image restoration;Tools;Distortion;Task analysis;Transform coding;Complexity theory","INSPEC Controlled Terms":"image restoration;learning (artificial intelligence)","INSPEC Non-Controlled Terms":"small-scale convolutional networks;RL-Restore;corrupted image;stepwise reward function;human-designed networks;image restoration;deep reinforcement learning","Mesh_Terms":"","Article Citation Count":"35","Patent Citation Count":"","Reference Count":"45","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Iterative MAP and ML Estimations for Image Segmentation","Authors":"Shifeng Chen; Liangliang Cao; Jianzhuang Liu; Xiaoou Tang","Author Affiliations":"Dept. of IE, The Chinese University of Hong Kong. sfchen5@ie.cuhk.edu.hk; Dept. of ECE, University of Illinois at Urbana-Champaign. cao4@uiuc.edu; Dept. of IE, The Chinese University of Hong Kong. jzliu@ie.cuhk.edu.hk; Dept. of IE, The Chinese University of Hong Kong; Microsoft Research Asia, Beijing, China. xitang@microsoft.com","Publication Title":"2007 IEEE Conference on Computer Vision and Pattern Recognition","Date Added To Xplore":"","Year":"2007","Volume":"","Issue":"","Start Page":"1","End Page":"6","Abstract":"Image segmentation plays an important role in computer vision and image analysis. In this paper, the segmentation problem is formulated as a labeling problem under a probability maximization framework. To estimate the label configuration, an iterative optimization scheme is proposed to alternately carry out the maximum a posteriori (MAP) estimation and the maximum-likelihood (ML) estimation. The MAP estimation problem is modeled with Markov random fields (MRFs). A graph-cut algorithm is used to find the solution to the MAP-MRF estimation. The ML estimation is achieved by finding the means of region features. Our algorithm can automatically segment an image into regions with relevant textures or colors without the need to know the number of regions in advance. In addition, under the same framework, it can be extended to another algorithm that extracts objects of a particular class from a group of images. Extensive experiments have shown the effectiveness of our approach.","ISSN":"","ISBNs":"","DOI":"10.1109/CVPR.2007.383007","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270032","Author Keywords":"","IEEE Terms":"Maximum likelihood estimation;Image segmentation;Computer vision;Iterative algorithms;Clustering algorithms;Asia;Image color analysis;Image texture analysis;Labeling;Markov random fields","INSPEC Controlled Terms":"computer vision;feature extraction;graph theory;image colour analysis;image segmentation;image texture;iterative methods;Markov processes;maximum likelihood estimation;optimisation;probability;random processes","INSPEC Non-Controlled Terms":"iterative method;image segmentation;computer vision;labeling problem;probability maximization framework;optimization;maximum a posteriori estimation;maximum-likelihood estimation;Markov random field;graph-cut algorithm;image texture;image color analysis;object extraction","Mesh_Terms":"","Article Citation Count":"14","Patent Citation Count":"","Reference Count":"17","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"CVPR"},{"Document Title":"Mining Motion Atoms and Phrases for Complex Action Recognition","Authors":" Limin Wang; Yu Qiao; Xiaoou Tang","Author Affiliations":"Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Shenzhen key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","Publication Title":"2013 IEEE International Conference on Computer Vision","Date Added To Xplore":"","Year":"2013","Volume":"","Issue":"","Start Page":"2680","End Page":"2687","Abstract":"This paper proposes motion atom and phrase as a mid-level temporal ``part'' for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discover a set of representative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representative power. We introduce a bottom-up phrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.","ISSN":"","ISBNs":"","DOI":"10.1109/ICCV.2013.333","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6751444","Author Keywords":"action recognition;mid-level representation","IEEE Terms":"Motion segmentation;Training;Hidden Markov models;Support vector machines;Image segmentation;Correlation;Equations","INSPEC Controlled Terms":"data mining;image classification;pattern clustering;video signal processing","INSPEC Non-Controlled Terms":"UCF50;Olympic sports;complex action dataset;mining task;greedy selection method;bottom-up phrase construction algorithm;effective motion phrase;discriminative clustering method;action video;motion information;complex action classification;midlevel temporal part;complex action recognition;phrases mining;motion atoms mining","Mesh_Terms":"","Article Citation Count":"81","Patent Citation Count":"","Reference Count":"30","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"ICCV"},{"Document Title":"Range Loss for Deep Face Recognition with Long-Tailed Training Data","Authors":" Xiao Zhang; Zhiyuan Fang; Yandong Wen; Zhifeng Li; Yu Qiao","Author Affiliations":"Guangdong Provincial Key Lab. of Comput. Vision & Vitrual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Guangdong Provincial Key Lab. of Comput. Vision & Vitrual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Tencent AI Lab., Shenzhen, China; NA; Guangdong Provincial Key Lab. of Comput. Vision & Vitrual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China","Publication Title":"2017 IEEE International Conference on Computer Vision (ICCV)","Date Added To Xplore":"","Year":"2017","Volume":"","Issue":"","Start Page":"5419","End Page":"5428","Abstract":"Deep convolutional neural networks have achieved significant improvements on face recognition task due to their ability to learn highly discriminative features from tremendous amounts of face images. Many large scale face datasets exhibit long-tail distribution where a small number of entities (persons) have large number of face images while a large number of persons only have very few face samples (long tail). Most of the existing works alleviate this problem by simply cutting the tailed data and only keep identities with enough number of examples. Unlike these work, this paper investigated how long-tailed data impact the training of face CNNs and develop a novel loss function, called range loss, to effectively utilize the tailed data in training process. More specifically, range loss is designed to reduce overall intrapersonal variations while enlarge interpersonal differences simultaneously. Extensive experiments on two face recognition benchmarks, Labeled Faces in the Wild (LFW) [11] and YouTube Faces (YTF) [33], demonstrate the effectiveness of the proposed range loss in overcoming the long tail effect, and show the good generalization ability of the proposed methods.","ISSN":"","ISBNs":"","DOI":"10.1109/ICCV.2017.578","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237840","Author Keywords":"","IEEE Terms":"Face;Training;Face recognition;Data models;Training data;Computer vision","INSPEC Controlled Terms":"convolution;face recognition;neural nets","INSPEC Non-Controlled Terms":"Labeled Faces in the Wild;long-tail distribution;face images;highly discriminative features;face recognition task;convolutional neural networks;long-tailed training data;deep face recognition;face recognition benchmarks;face CNNs","Mesh_Terms":"","Article Citation Count":"89","Patent Citation Count":"","Reference Count":"38","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"ICCV"},{"Document Title":"RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos","Authors":"Wenbin Du; Yali Wang; Yu Qiao","Author Affiliations":"Shenzhen Coll. of Adv. Technol., Univ. of Chinese Acad. of Sci., Shenzhen, China; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China","Publication Title":"2017 IEEE International Conference on Computer Vision (ICCV)","Date Added To Xplore":"","Year":"2017","Volume":"","Issue":"","Start Page":"3745","End Page":"3754","Abstract":"Recent studies demonstrate the effectiveness of Recurrent Neural Networks (RNNs) for action recognition in videos. However, previous works mainly utilize video-level category as supervision to train RNNs, which may prohibit RNNs to learn complex motion structures along time. In this paper, we propose a recurrent pose-attention network (RPAN) to address this challenge, where we introduce a novel pose-attention mechanism to adaptively learn pose-related features at every time-step action prediction of RNNs. More specifically, we make three main contributions in this paper. Firstly, unlike previous works on pose-related action recognition, our RPAN is an end-to-end recurrent network which can exploit important spatial-temporal evolutions of human pose to assist action recognition in a unified framework. Secondly, instead of learning individual human-joint features separately, our pose-attention mechanism learns robust human-part features by sharing attention parameters partially on the semantically-related human joints. These human-part features are then fed into the human-part pooling layer to construct a highly-discriminative pose-related representation for temporal action modeling. Thirdly, one important byproduct of our RPAN is pose estimation in videos, which can be used for coarse pose annotation in action videos. We evaluate the proposed RPAN quantitatively and qualitatively on two popular benchmarks, i.e., Sub-JHMDB and PennAction. Experimental results show that RPAN outperforms the recent state-of-the-art methods on these challenging datasets.","ISSN":"","ISBNs":"","DOI":"10.1109/ICCV.2017.402","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237664","Author Keywords":"","IEEE Terms":"Videos;Robustness;Heating systems;Computer vision;Recurrent neural networks;Pose estimation;Image recognition","INSPEC Controlled Terms":"feature extraction;image motion analysis;pose estimation;recurrent neural nets;video signal processing","INSPEC Non-Controlled Terms":"RPAN;Recurrent Neural Networks;RNNs;video-level category;complex motion structures;pose-attention mechanism;time-step action prediction;pose-related action recognition;human-joint features;robust human-part features;attention parameters;human-part pooling layer;temporal action modeling;action videos;recurrent pose-attention network","Mesh_Terms":"","Article Citation Count":"55","Patent Citation Count":"","Reference Count":"54","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"ICCV"},{"Document Title":"Single Shot Text Detector with Regional Attention","Authors":" Pan He; Weilin Huang; Tong He; Qile Zhu; Yu Qiao; Xiaolin Li","Author Affiliations":"Nat. Sci. Found. Center for Big Learning, Univ. of Florida, Gainesville, FL, USA; Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Nat. Sci. Found. Center for Big Learning, Univ. of Florida, Gainesville, FL, USA; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Nat. Sci. Found. Center for Big Learning, Univ. of Florida, Gainesville, FL, USA","Publication Title":"2017 IEEE International Conference on Computer Vision (ICCV)","Date Added To Xplore":"","Year":"2017","Volume":"","Issue":"","Start Page":"3066","End Page":"3074","Abstract":"We present a novel single-shot text detector that directly outputs word-level bounding boxes in a natural image. We propose an attention mechanism which roughly identifies text regions via an automatically learned attentional map. This substantially suppresses background interference in the convolutional features, which is the key to producing accurate inference of words, particularly at extremely small sizes. This results in a single model that essentially works in a coarse-to-fine manner. It departs from recent FCN-based text detectors which cascade multiple FCN models to achieve an accurate prediction. Furthermore, we develop a hierarchical inception module which efficiently aggregates multi-scale inception features. This enhances local details, and also encodes strong context information, allowing the detector to work reliably on multi-scale and multi-orientation text with single-scale images. Our text detector achieves an F-measure of 77% on the ICDAR 2015 benchmark, advancing the state-of-the-art results in [18, 28]. Demo is available at: http://sstd.whuang.org/.","ISSN":"","ISBNs":"","DOI":"10.1109/ICCV.2017.331","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237593","Author Keywords":"","IEEE Terms":"Detectors;Convolutional codes;Predictive models;Heating systems;Computer vision;Interference","INSPEC Controlled Terms":"convolution;feature extraction;natural scenes;neural nets;text detection","INSPEC Non-Controlled Terms":"single shot text detector;regional attention;word-level bounding boxes;natural image;attention mechanism;text regions;background interference;convolutional features;hierarchical inception module;single-scale images;attentional map;FCN-based text detectors;multiscale inception features;Fully Convolutional Network","Mesh_Terms":"","Article Citation Count":"115","Patent Citation Count":"","Reference Count":"38","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"ICCV"},{"Document Title":"Detecting Faces Using Inside Cascaded Contextual CNN","Authors":" Kaipeng Zhang; Zhanpeng Zhang; Hao Wang; Zhifeng Li; Yu Qiao; Wei Liu","Author Affiliations":"NA; NA; NA; NA; Guangdong Provincial Key Lab. of Comput. Vision & Virtual Reality Technol., Shenzhen Inst. of Adv. Technol., Shenzhen, China; NA","Publication Title":"2017 IEEE International Conference on Computer Vision (ICCV)","Date Added To Xplore":"","Year":"2017","Volume":"","Issue":"","Start Page":"3190","End Page":"3198","Abstract":"Deep Convolutional Neural Networks (CNNs) achieve substantial improvements in face detection in the wild. Classical CNN-based face detection methods simply stack successive layers of filters where an input sample should pass through all layers before reaching a face/non-face decision. Inspired by the fact that for face detection, filters in deeper layers can discriminate between difficult face/non-face samples while those in shallower layers can efficiently reject simple non-face samples, we propose Inside Cascaded Structure that introduces face/non-face classifiers at different layers within the same CNN. In the training phase, we propose data routing mechanism which enables different layers to be trained by different types of samples, and thus deeper layers can focus on handling more difficult samples compared with traditional architecture. In addition, we introduce a two-stream contextual CNN architecture that leverages body part information adaptively to enhance face detection. Extensive experiments on the challenging FD-DB and WIDER FACE benchmarks demonstrate that our method achieves competitive accuracy to the state-of-the-art techniques while keeps real time performance.","ISSN":"","ISBNs":"","DOI":"10.1109/ICCV.2017.344","Funding Information":"","PDF Link":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237606","Author Keywords":"","IEEE Terms":"Face detection;Training;Integrated circuits;Feature extraction;Routing;Computer architecture;Testing","INSPEC Controlled Terms":"convolution;face recognition;image classification;image enhancement;learning (artificial intelligence);neural nets","INSPEC Non-Controlled Terms":"detection methods;deeper layers;shallower layers;face samples;face decision;nonface decision;inside cascaded contextual CNN;nonface samples;face classifiers;nonface classifiers;deep convolutional neural networks;face detection enhancement;WIDER FACE benchmarks;two-stream contextual CNN architecture","Mesh_Terms":"","Article Citation Count":"25","Patent Citation Count":"","Reference Count":"32","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"IEEE","Document Identifier":"IEEE Conferences","Venues":"ICCV"},{"Document Title":"RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution","Authors":"Wenlong Zhang; Yihao Liu; Chao Dong; Yu Qiao","Author Affiliations":"","Publication Title":"2019 IEEE International Conference on Computer Vision (ICCV)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"3","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ICCV"},{"Document Title":"LAP-Net: Level-Aware Progressive Network for Image Dehazing","Authors":"Yunan Li; Qiguang Miao; Wanli Ouyang; Zhenxin Ma; Huijuan Fang; Chao Dong; Yining Quan","Author Affiliations":"","Publication Title":"2019 IEEE International Conference on Computer Vision (ICCV)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"LAP-Net Level-Aware Progressive Network for Image Dehazing","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"0","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ICCV"},{"Document Title":"DF2Net: A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction","Authors":"Xiaoxing Zeng; Xiaojiang Peng; Yu Qiao","Author Affiliations":"","Publication Title":"2019 IEEE International Conference on Computer Vision (ICCV)","Date Added To Xplore":"","Year":"2019","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"0","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ICCV"},{"Document Title":"Super-Identity Convolutional Neural Network for Face Hallucination","Authors":"Kaipeng Zhang;Zhanpeng Zhang;Chia-Wen Cheng;Winston H. Hsu;Yu Qiao;Wei Liu;Tong Zhang","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-030-01252-6","Funding Information":"","PDF Link":"","Author Keywords":"Face hallucination;Super identity;Domain-integrated training;Convolutional neural networks ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"18","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Find and Focus: Retrieve and Localize Video Events with Natural Language Queries","Authors":"Dian Shao;Yu Xiong;Yue Zhao;Qingqiu Huang;Yu Qiao;Dahua Lin","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-030-01240-3","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"7","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition","Authors":"Limin Wang;Yuanjun Xiong;Zhe Wang;Yu Qiao;Dahua Lin;Xiaoou Tang;Luc Van Gool","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-46484-8_2","Funding Information":"","PDF Link":"","Author Keywords":"Action recognition; Temporal segment networks; Good practices; ConvNets ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"1168","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Detecting Text in Natural Image with Connectionist Text Proposal Network","Authors":" Zhi Tian; Weilin Huang; Tong He; Pan He; Yu Qiao;","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-46484-8_4","Funding Information":"","PDF Link":"","Author Keywords":"Scene text detection; Convolutional network; Recurrent neural network; Anchor mechanism ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"326","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"A Discriminative Feature Learning Approach for Deep Face Recognition","Authors":"Yandong Wen; Kaipeng Zhang; Zhifeng Li; Yu Qiao","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-46478-7","Funding Information":"","PDF Link":"","Author Keywords":"Convolutional neural networks; Face recognition; Discriminative feature learning; Center loss ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"1189","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Robust Scene Text Detection with Convolution Neural Network Induced MSER Trees","Authors":" Weilin Huang; Yu Qiao; Xiaoou Tang","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2014","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-10593-2","Funding Information":"","PDF Link":"","Author Keywords":"Maximally Stable Extremal Regions (MSERs); convolutional neural network (CNN); text-like outliers; sliding-window ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"284","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Video Action Detection with Relational Dynamic-Poselets","Authors":" Limin Wang; Yu Qiao; Xiaoou Tang","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2014","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"Action detection is of great importance in understanding human motion from video. Compared with action recognition, it not only recognizes action type, but also localizes its spatiotemporal extent. This paper presents a relational model for action detection, which first decomposes human action into temporal ��key poses�� and then further into spatial ��action parts��. Specifically, we start by clustering cuboids around each human joint into dynamic-poselets using a new descriptor. The cuboids from the same cluster share consistent geometric and dynamic structure, and each cluster acts as a mixture of body parts. We then propose a sequential skeleton model to capture the relations among dynamic-poselets. This model unifies the tasks of learning the composites of mixture dynamic-poselets, the spatiotemporal structures of action parts, and the local model for each action part in a single framework. Our model not only allows to localize the action in a video stream, but also enables a detailed pose estimation of an actor. We formulate the model learning problem in a structured SVM framework and speed up model inference by dynamic programming. We conduct experiments on three challenging action detection datasets: the MSR-II dataset, the UCF Sports dataset, and the JHMDB dataset. The results show that our method achieves superior performance to the state-of-the-art methods on these datasets.","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-10602-1","Funding Information":"","PDF Link":"","Author Keywords":"Action detection; dynamic-poselet; sequential skeleton model","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"105","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Action Recognition with Stacked Fisher Vectors","Authors":"Xiaojiang Peng;Changqing Zou;Yu Qiao;Qiang Peng","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2014","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-10602-1_38","Funding Information":"","PDF Link":"","Author Keywords":"Action recognition; Fisher vectors; stacked Fisher vectors; max-margin dimensionality reduction ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"312","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Boosting VLAD with Supervised Dictionary Learning and High-Order Statistics","Authors":"Xiaojiang Peng; Limin Wang; Yu Qiao; Qiang Peng","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2014","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-10578-9","Funding Information":"","PDF Link":"","Author Keywords":"Visual Word; Action Recognition; Local Descriptor; Sparse Code; Convolutional Neural Network","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"68","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters","Authors":"Yifan Xu;Tianqi Fan;Mingye Xu;Long Zeng;Yu Qiao","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-030-01237-3_6","Funding Information":"","PDF Link":"","Author Keywords":"Convolutional neural network; Parametrized convolutional filters; Point clouds ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"80","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Learning a Deep Convolutional Network for Image Super-Resolution","Authors":"Chao Dong;Chen Change Loy;Kaiming He;Xiaoou Tang","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2014","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-10593-2_13","Funding Information":"","PDF Link":"","Author Keywords":"Super-resolution; deep convolutional neural networks ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"1940","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Accelerating the super-resolution convolutional neural network","Authors":"Chao Dong;Chen Change Loy;Xiaoou Tang","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-46475-6_25","Funding Information":"","PDF Link":"","Author Keywords":"Mapping Layer; Deep Model; Interpolation Kernel; Convolution Filter; Convolution Layer ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"734","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Esrgan: Enhanced super-resolution generative adversarial networks","Authors":"Xintao Wang;Ke Yu;Shixiang Wu;Jinjin Gu;Yihao Liu;Chao Dong;Yu Qiao;Chen Change Loy","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2018","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-030-11021-5_5","Funding Information":"","PDF Link":"","Author Keywords":"","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"199","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"},{"Document Title":"Multi-region two-stream R-CNN for action detection","Authors":"Xiaojiang Peng;Cordelia Schmid","Author Affiliations":"","Publication Title":"","Date Added To Xplore":"","Year":"2016","Volume":"","Issue":"","Start Page":"","End Page":"","Abstract":"","ISSN":"","ISBNs":"","DOI":"10.1007/978-3-319-46493-0_45","Funding Information":"","PDF Link":"","Author Keywords":"Action detection; Faster R-CNN; Multi-region CNNs; Two stream R-CNN ","IEEE Terms":"","INSPEC Controlled Terms":"","INSPEC Non-Controlled Terms":"","Mesh_Terms":"","Article Citation Count":"186","Patent Citation Count":"","Reference Count":"","License":"","Online Date":"","Issue Date":"","Meeting Date":"","Publisher":"","Document Identifier":"","Venues":"ECCV"}];
var figure2data=
{38:[{"size":50505,"src":"./images/10.1007978-3-030-01237-3_6-Figure2-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure2-1.png","fratio":1.4756756756756757,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":22608,"src":"./images/10.1007978-3-030-01237-3_6-Figure6-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure6-1.png","fratio":4.361111111111111,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":21978,"src":"./images/10.1007978-3-030-01237-3_6-Figure7-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure7-1.png","fratio":4.013513513513513,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":23760,"src":"./images/10.1007978-3-030-01237-3_6-Figure1-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure1-1.png","fratio":3.7125,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":42864,"src":"./images/10.1007978-3-030-01237-3_6-Table1-1.png","figureid":"10.1007978-3-030-01237-3_6-Table1-1.png","fratio":2.1560283687943262,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":46472,"src":"./images/10.1007978-3-030-01237-3_6-Figure5-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure5-1.png","fratio":2.1216216216216215,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":29841,"src":"./images/10.1007978-3-030-01237-3_6-Figure11-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure11-1.png","fratio":3.942528735632184,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":60207,"src":"./images/10.1007978-3-030-01237-3_6-Figure12-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure12-1.png","fratio":1.7978142076502732,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":31616,"src":"./images/10.1007978-3-030-01237-3_6-Figure10-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure10-1.png","fratio":2.923076923076923,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":22968,"src":"./images/10.1007978-3-030-01237-3_6-Table3-1.png","figureid":"10.1007978-3-030-01237-3_6-Table3-1.png","fratio":5.2727272727272725,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":25116,"src":"./images/10.1007978-3-030-01237-3_6-Figure3-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure3-1.png","fratio":3.5595238095238093,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":25984,"src":"./images/10.1007978-3-030-01237-3_6-Table2-1.png","figureid":"10.1007978-3-030-01237-3_6-Table2-1.png","fratio":2.0714285714285716,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":25612,"src":"./images/10.1007978-3-030-01237-3_6-Figure8-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure8-1.png","fratio":4.434210526315789,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":14942,"src":"./images/10.1007978-3-030-01237-3_6-Figure9-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure9-1.png","fratio":3.8870967741935485,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},{"size":32334,"src":"./images/10.1007978-3-030-01237-3_6-Figure4-1.png","figureid":"10.1007978-3-030-01237-3_6-Figure4-1.png","fratio":3.107843137254902,"textp":4463143,"paperid":38,"fignums":15,"pagenums":16},],30:[{"size":40716,"src":"./images/10.1007978-3-030-01240-3-Figure3-1.png","figureid":"10.1007978-3-030-01240-3-Figure3-1.png","fratio":3.0258620689655173,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":40320,"src":"./images/10.1007978-3-030-01240-3-Figure4-1.png","figureid":"10.1007978-3-030-01240-3-Figure4-1.png","fratio":2.4609375,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":40635,"src":"./images/10.1007978-3-030-01240-3-Figure1-1.png","figureid":"10.1007978-3-030-01240-3-Figure1-1.png","fratio":2.441860465116279,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":27160,"src":"./images/10.1007978-3-030-01240-3-Table1-1.png","figureid":"10.1007978-3-030-01240-3-Table1-1.png","fratio":2.88659793814433,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":15340,"src":"./images/10.1007978-3-030-01240-3-Table2-1.png","figureid":"10.1007978-3-030-01240-3-Table2-1.png","fratio":4.406779661016949,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":103874,"src":"./images/10.1007978-3-030-01240-3-Figure5-1.png","figureid":"10.1007978-3-030-01240-3-Figure5-1.png","fratio":1.0739549839228295,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":14304,"src":"./images/10.1007978-3-030-01240-3-Table6-1.png","figureid":"10.1007978-3-030-01240-3-Table6-1.png","fratio":6.208333333333333,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":8568,"src":"./images/10.1007978-3-030-01240-3-Table7-1.png","figureid":"10.1007978-3-030-01240-3-Table7-1.png","fratio":6.611111111111111,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":26796,"src":"./images/10.1007978-3-030-01240-3-Table5-1.png","figureid":"10.1007978-3-030-01240-3-Table5-1.png","fratio":3.7976190476190474,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":34170,"src":"./images/10.1007978-3-030-01240-3-Table4-1.png","figureid":"10.1007978-3-030-01240-3-Table4-1.png","fratio":3.284313725490196,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":12744,"src":"./images/10.1007978-3-030-01240-3-Table3-1.png","figureid":"10.1007978-3-030-01240-3-Table3-1.png","fratio":3.6610169491525424,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},{"size":27776,"src":"./images/10.1007978-3-030-01240-3-Figure2-1.png","figureid":"10.1007978-3-030-01240-3-Figure2-1.png","fratio":2.2142857142857144,"textp":4463143,"paperid":30,"fignums":12,"pagenums":17},],29:[{"size":42449,"src":"./images/10.1007978-3-030-01252-6-Table1-1.png","figureid":"10.1007978-3-030-01252-6-Table1-1.png","fratio":0.8237885462555066,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":35741,"src":"./images/10.1007978-3-030-01252-6-Figure5-1.png","figureid":"10.1007978-3-030-01252-6-Figure5-1.png","fratio":3.3689320388349513,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":36192,"src":"./images/10.1007978-3-030-01252-6-Figure7-1.png","figureid":"10.1007978-3-030-01252-6-Figure7-1.png","fratio":3.3461538461538463,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":47676,"src":"./images/10.1007978-3-030-01252-6-Figure1-1.png","figureid":"10.1007978-3-030-01252-6-Figure1-1.png","fratio":2.54014598540146,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":37932,"src":"./images/10.1007978-3-030-01252-6-Figure3-1.png","figureid":"10.1007978-3-030-01252-6-Figure3-1.png","fratio":3.1926605504587156,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":143175,"src":"./images/10.1007978-3-030-01252-6-Figure6-1.png","figureid":"10.1007978-3-030-01252-6-Figure6-1.png","fratio":0.8313253012048193,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":10044,"src":"./images/10.1007978-3-030-01252-6-Table4-1.png","figureid":"10.1007978-3-030-01252-6-Table4-1.png","fratio":7.75,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":14570,"src":"./images/10.1007978-3-030-01252-6-Table5-1.png","figureid":"10.1007978-3-030-01252-6-Table5-1.png","fratio":6.595744680851064,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":64032,"src":"./images/10.1007978-3-030-01252-6-Figure4-1.png","figureid":"10.1007978-3-030-01252-6-Figure4-1.png","fratio":1.891304347826087,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":12864,"src":"./images/10.1007978-3-030-01252-6-Table2-1.png","figureid":"10.1007978-3-030-01252-6-Table2-1.png","fratio":5.583333333333333,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":13677,"src":"./images/10.1007978-3-030-01252-6-Table3-1.png","figureid":"10.1007978-3-030-01252-6-Table3-1.png","fratio":6.191489361702128,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":58116,"src":"./images/10.1007978-3-030-01252-6-Figure2-1.png","figureid":"10.1007978-3-030-01252-6-Figure2-1.png","fratio":2.0838323353293413,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},{"size":16779,"src":"./images/10.1007978-3-030-01252-6-Table6-1.png","figureid":"10.1007978-3-030-01252-6-Table6-1.png","fratio":7.595744680851064,"textp":4463143,"paperid":29,"fignums":13,"pagenums":18},],41:[{"size":53956,"src":"./images/10.1007978-3-030-11021-5_5-Figure1-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure1-1.png","fratio":2.0060975609756095,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":17100,"src":"./images/10.1007978-3-030-11021-5_5-Figure5-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure5-1.png","fratio":6.84,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":159732,"src":"./images/10.1007978-3-030-11021-5_5-Figure7-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure7-1.png","fratio":0.6572008113590264,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":70794,"src":"./images/10.1007978-3-030-11021-5_5-Figure10-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure10-1.png","fratio":1.6521739130434783,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":37050,"src":"./images/10.1007978-3-030-11021-5_5-Figure2-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure2-1.png","fratio":2.1923076923076925,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":34505,"src":"./images/10.1007978-3-030-11021-5_5-Figure9-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure9-1.png","fratio":3.2524271844660193,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":36934,"src":"./images/10.1007978-3-030-11021-5_5-Figure6-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure6-1.png","fratio":2.652542372881356,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":170471,"src":"./images/10.1007978-3-030-11021-5_5-Figure8-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure8-1.png","fratio":0.6901408450704225,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":20923,"src":"./images/10.1007978-3-030-11021-5_5-Figure4-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure4-1.png","fratio":5.622950819672131,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},{"size":17384,"src":"./images/10.1007978-3-030-11021-5_5-Figure3-1.png","figureid":"10.1007978-3-030-11021-5_5-Figure3-1.png","fratio":6.188679245283019,"textp":4463143,"paperid":41,"fignums":10,"pagenums":17},],37:[{"size":86520,"src":"./images/10.1007978-3-319-10578-9-Figure2-1.png","figureid":"10.1007978-3-319-10578-9-Figure2-1.png","fratio":1.1035714285714286,"textp":4463143,"paperid":37,"fignums":6,"pagenums":15},{"size":87048,"src":"./images/10.1007978-3-319-10578-9-Table1-1.png","figureid":"10.1007978-3-319-10578-9-Table1-1.png","fratio":1.4153225806451613,"textp":4463143,"paperid":37,"fignums":6,"pagenums":15},{"size":32017,"src":"./images/10.1007978-3-319-10578-9-Figure1-1.png","figureid":"10.1007978-3-319-10578-9-Figure1-1.png","fratio":3.1386138613861387,"textp":4463143,"paperid":37,"fignums":6,"pagenums":15},{"size":37260,"src":"./images/10.1007978-3-319-10578-9-Figure3-1.png","figureid":"10.1007978-3-319-10578-9-Figure3-1.png","fratio":2.8173913043478263,"textp":4463143,"paperid":37,"fignums":6,"pagenums":15},{"size":30268,"src":"./images/10.1007978-3-319-10578-9-Table3-1.png","figureid":"10.1007978-3-319-10578-9-Table3-1.png","fratio":3.5760869565217392,"textp":4463143,"paperid":37,"fignums":6,"pagenums":15},{"size":24992,"src":"./images/10.1007978-3-319-10578-9-Table2-1.png","figureid":"10.1007978-3-319-10578-9-Table2-1.png","fratio":4.957746478873239,"textp":4463143,"paperid":37,"fignums":6,"pagenums":15},],34:[{"size":35292,"src":"./images/10.1007978-3-319-10593-2-Figure1-1.png","figureid":"10.1007978-3-319-10593-2-Figure1-1.png","fratio":3.392156862745098,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},{"size":49104,"src":"./images/10.1007978-3-319-10593-2-Figure3-1.png","figureid":"10.1007978-3-319-10593-2-Figure3-1.png","fratio":2.3680555555555554,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},{"size":39578,"src":"./images/10.1007978-3-319-10593-2-Table1-1.png","figureid":"10.1007978-3-319-10593-2-Table1-1.png","fratio":1.6688311688311688,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},{"size":33667,"src":"./images/10.1007978-3-319-10593-2-Table2-1.png","figureid":"10.1007978-3-319-10593-2-Table2-1.png","fratio":1.9618320610687023,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},{"size":116622,"src":"./images/10.1007978-3-319-10593-2-Figure5-1.png","figureid":"10.1007978-3-319-10593-2-Figure5-1.png","fratio":1.002932551319648,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},{"size":26335,"src":"./images/10.1007978-3-319-10593-2-Figure6-1.png","figureid":"10.1007978-3-319-10593-2-Figure6-1.png","fratio":1.991304347826087,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},{"size":58652,"src":"./images/10.1007978-3-319-10593-2-Figure4-1.png","figureid":"10.1007978-3-319-10593-2-Figure4-1.png","fratio":1.9825581395348837,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},{"size":87380,"src":"./images/10.1007978-3-319-10593-2-Figure2-1.png","figureid":"10.1007978-3-319-10593-2-Figure2-1.png","fratio":1.3229571984435797,"textp":4463143,"paperid":34,"fignums":8,"pagenums":15},],39:[{"size":45240,"src":"./images/10.1007978-3-319-10593-2_13-Figure3-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure3-1.png","fratio":2.6769230769230767,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":21624,"src":"./images/10.1007978-3-319-10593-2_13-Figure4-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure4-1.png","fratio":1.9245283018867925,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":22344,"src":"./images/10.1007978-3-319-10593-2_13-Figure5-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure5-1.png","fratio":3.8684210526315788,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":63133,"src":"./images/10.1007978-3-319-10593-2_13-Figure1-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure1-1.png","fratio":1.5320197044334976,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":44936,"src":"./images/10.1007978-3-319-10593-2_13-Table1-1.png","figureid":"10.1007978-3-319-10593-2_13-Table1-1.png","fratio":2.394160583941606,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":36630,"src":"./images/10.1007978-3-319-10593-2_13-Table2-1.png","figureid":"10.1007978-3-319-10593-2_13-Table2-1.png","fratio":3.0272727272727273,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":46452,"src":"./images/10.1007978-3-319-10593-2_13-Figure10-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure10-1.png","fratio":1.860759493670886,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":47040,"src":"./images/10.1007978-3-319-10593-2_13-Figure9-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure9-1.png","fratio":1.8375,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":47040,"src":"./images/10.1007978-3-319-10593-2_13-Figure8-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure8-1.png","fratio":1.8375,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":46746,"src":"./images/10.1007978-3-319-10593-2_13-Figure7-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure7-1.png","fratio":1.849056603773585,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":9324,"src":"./images/10.1007978-3-319-10593-2_13-Table3-1.png","figureid":"10.1007978-3-319-10593-2_13-Table3-1.png","fratio":6.8108108108108105,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":28980,"src":"./images/10.1007978-3-319-10593-2_13-Figure6-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure6-1.png","fratio":2.6285714285714286,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},{"size":42558,"src":"./images/10.1007978-3-319-10593-2_13-Figure2-1.png","figureid":"10.1007978-3-319-10593-2_13-Figure2-1.png","fratio":2.813008130081301,"textp":4463143,"paperid":39,"fignums":13,"pagenums":16},],35:[{"size":14382,"src":"./images/10.1007978-3-319-10602-1-Table1-1.png","figureid":"10.1007978-3-319-10602-1-Table1-1.png","fratio":6.51063829787234,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},{"size":30576,"src":"./images/10.1007978-3-319-10602-1-Figure4-1.png","figureid":"10.1007978-3-319-10602-1-Figure4-1.png","fratio":3.6923076923076925,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},{"size":48720,"src":"./images/10.1007978-3-319-10602-1-Figure1-1.png","figureid":"10.1007978-3-319-10602-1-Figure1-1.png","fratio":2.4857142857142858,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},{"size":67665,"src":"./images/10.1007978-3-319-10602-1-Figure3-1.png","figureid":"10.1007978-3-319-10602-1-Figure3-1.png","fratio":1.7794871794871794,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},{"size":28560,"src":"./images/10.1007978-3-319-10602-1-Figure6-1.png","figureid":"10.1007978-3-319-10602-1-Figure6-1.png","fratio":3.9529411764705884,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},{"size":88138,"src":"./images/10.1007978-3-319-10602-1-Figure7-1.png","figureid":"10.1007978-3-319-10602-1-Figure7-1.png","fratio":1.3661417322834646,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},{"size":28645,"src":"./images/10.1007978-3-319-10602-1-Figure5-1.png","figureid":"10.1007978-3-319-10602-1-Figure5-1.png","fratio":3.9647058823529413,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},{"size":37584,"src":"./images/10.1007978-3-319-10602-1-Figure2-1.png","figureid":"10.1007978-3-319-10602-1-Figure2-1.png","fratio":3.2222222222222223,"textp":4463143,"paperid":35,"fignums":8,"pagenums":16},],36:[{"size":38940,"src":"./images/10.1007978-3-319-10602-1_38-Figure2-1.png","figureid":"10.1007978-3-319-10602-1_38-Figure2-1.png","fratio":2.7966101694915255,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":25680,"src":"./images/10.1007978-3-319-10602-1_38-Table2-1.png","figureid":"10.1007978-3-319-10602-1_38-Table2-1.png","fratio":4.0125,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":50820,"src":"./images/10.1007978-3-319-10602-1_38-Figure1-1.png","figureid":"10.1007978-3-319-10602-1_38-Figure1-1.png","fratio":2.142857142857143,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":51240,"src":"./images/10.1007978-3-319-10602-1_38-Figure4-1.png","figureid":"10.1007978-3-319-10602-1_38-Figure4-1.png","fratio":1.8154761904761905,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":31719,"src":"./images/10.1007978-3-319-10602-1_38-Figure5-1.png","figureid":"10.1007978-3-319-10602-1_38-Figure5-1.png","fratio":3.3711340206185567,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":26910,"src":"./images/10.1007978-3-319-10602-1_38-Table3-1.png","figureid":"10.1007978-3-319-10602-1_38-Table3-1.png","fratio":4.423076923076923,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":45540,"src":"./images/10.1007978-3-319-10602-1_38-Figure3-1.png","figureid":"10.1007978-3-319-10602-1_38-Figure3-1.png","fratio":2.391304347826087,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":70656,"src":"./images/10.1007978-3-319-10602-1_38-Figure6-1.png","figureid":"10.1007978-3-319-10602-1_38-Figure6-1.png","fratio":1.078125,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},{"size":13344,"src":"./images/10.1007978-3-319-10602-1_38-Table1-1.png","figureid":"10.1007978-3-319-10602-1_38-Table1-1.png","fratio":5.791666666666667,"textp":4463143,"paperid":36,"fignums":9,"pagenums":15},],40:[{"size":37100,"src":"./images/10.1007978-3-319-46475-6_25-Table4-1.png","figureid":"10.1007978-3-319-46475-6_25-Table4-1.png","fratio":3.30188679245283,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":46768,"src":"./images/10.1007978-3-319-46475-6_25-Figure7-1.png","figureid":"10.1007978-3-319-46475-6_25-Figure7-1.png","fratio":1.8734177215189873,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":32524,"src":"./images/10.1007978-3-319-46475-6_25-Figure4-1.png","figureid":"10.1007978-3-319-46475-6_25-Figure4-1.png","fratio":3.6808510638297873,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":37343,"src":"./images/10.1007978-3-319-46475-6_25-Table3-1.png","figureid":"10.1007978-3-319-46475-6_25-Table3-1.png","fratio":3.2616822429906542,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":25190,"src":"./images/10.1007978-3-319-46475-6_25-Figure1-1.png","figureid":"10.1007978-3-319-46475-6_25-Figure1-1.png","fratio":2.081818181818182,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":17056,"src":"./images/10.1007978-3-319-46475-6_25-Figure6-1.png","figureid":"10.1007978-3-319-46475-6_25-Figure6-1.png","fratio":2.5365853658536586,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":16116,"src":"./images/10.1007978-3-319-46475-6_25-Figure3-1.png","figureid":"10.1007978-3-319-46475-6_25-Figure3-1.png","fratio":3.485294117647059,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":17712,"src":"./images/10.1007978-3-319-46475-6_25-Table2-1.png","figureid":"10.1007978-3-319-46475-6_25-Table2-1.png","fratio":3.4166666666666665,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":27621,"src":"./images/10.1007978-3-319-46475-6_25-Figure5-1.png","figureid":"10.1007978-3-319-46475-6_25-Figure5-1.png","fratio":4.209876543209877,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":33712,"src":"./images/10.1007978-3-319-46475-6_25-Table1-1.png","figureid":"10.1007978-3-319-46475-6_25-Table1-1.png","fratio":3.510204081632653,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},{"size":42000,"src":"./images/10.1007978-3-319-46475-6_25-Figure2-1.png","figureid":"10.1007978-3-319-46475-6_25-Figure2-1.png","fratio":2.688,"textp":4463143,"paperid":40,"fignums":11,"pagenums":17},],33:[{"size":32376,"src":"./images/10.1007978-3-319-46478-7-Table2-1.png","figureid":"10.1007978-3-319-46478-7-Table2-1.png","fratio":2.491228070175439,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":41788,"src":"./images/10.1007978-3-319-46478-7-Figure1-1.png","figureid":"10.1007978-3-319-46478-7-Figure1-1.png","fratio":2.717741935483871,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":71530,"src":"./images/10.1007978-3-319-46478-7-Figure3-1.png","figureid":"10.1007978-3-319-46478-7-Figure3-1.png","fratio":1.3521739130434782,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":58474,"src":"./images/10.1007978-3-319-46478-7-Figure6-1.png","figureid":"10.1007978-3-319-46478-7-Figure6-1.png","fratio":1.953757225433526,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":39216,"src":"./images/10.1007978-3-319-46478-7-Table4-1.png","figureid":"10.1007978-3-319-46478-7-Table4-1.png","fratio":2.356589147286822,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":39936,"src":"./images/10.1007978-3-319-46478-7-Table3-1.png","figureid":"10.1007978-3-319-46478-7-Table3-1.png","fratio":2.4375,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":38150,"src":"./images/10.1007978-3-319-46478-7-Figure8-1.png","figureid":"10.1007978-3-319-46478-7-Figure8-1.png","fratio":3.2110091743119265,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":39776,"src":"./images/10.1007978-3-319-46478-7-Figure9-1.png","figureid":"10.1007978-3-319-46478-7-Figure9-1.png","fratio":3.1150442477876106,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":47460,"src":"./images/10.1007978-3-319-46478-7-Figure4-1.png","figureid":"10.1007978-3-319-46478-7-Figure4-1.png","fratio":2.4214285714285713,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":15360,"src":"./images/10.1007978-3-319-46478-7-Table1-1.png","figureid":"10.1007978-3-319-46478-7-Table1-1.png","fratio":6.666666666666667,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":54352,"src":"./images/10.1007978-3-319-46478-7-Figure7-1.png","figureid":"10.1007978-3-319-46478-7-Figure7-1.png","fratio":2.1772151898734178,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":41912,"src":"./images/10.1007978-3-319-46478-7-Figure5-1.png","figureid":"10.1007978-3-319-46478-7-Figure5-1.png","fratio":2.725806451612903,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},{"size":46761,"src":"./images/10.1007978-3-319-46478-7-Figure2-1.png","figureid":"10.1007978-3-319-46478-7-Figure2-1.png","fratio":2.2867132867132867,"textp":4463143,"paperid":33,"fignums":13,"pagenums":17},],31:[{"size":22788,"src":"./images/10.1007978-3-319-46484-8_2-Table2-1.png","figureid":"10.1007978-3-319-46484-8_2-Table2-1.png","fratio":1.9537037037037037,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":13771,"src":"./images/10.1007978-3-319-46484-8_2-Table3-1.png","figureid":"10.1007978-3-319-46484-8_2-Table3-1.png","fratio":6.23404255319149,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":30422,"src":"./images/10.1007978-3-319-46484-8_2-Figure2-1.png","figureid":"10.1007978-3-319-46484-8_2-Figure2-1.png","fratio":2.707547169811321,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":23450,"src":"./images/10.1007978-3-319-46484-8_2-Table1-1.png","figureid":"10.1007978-3-319-46484-8_2-Table1-1.png","fratio":4.785714285714286,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":56112,"src":"./images/10.1007978-3-319-46484-8_2-Figure3-1.png","figureid":"10.1007978-3-319-46484-8_2-Figure3-1.png","fratio":2.0119760479041915,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":52160,"src":"./images/10.1007978-3-319-46484-8_2-Table6-1.png","figureid":"10.1007978-3-319-46484-8_2-Table6-1.png","fratio":1.9631901840490797,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":20803,"src":"./images/10.1007978-3-319-46484-8_2-Table4-1.png","figureid":"10.1007978-3-319-46484-8_2-Table4-1.png","fratio":4.126760563380282,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":10676,"src":"./images/10.1007978-3-319-46484-8_2-Table5-1.png","figureid":"10.1007978-3-319-46484-8_2-Table5-1.png","fratio":9.235294117647058,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},{"size":41540,"src":"./images/10.1007978-3-319-46484-8_2-Figure1-1.png","figureid":"10.1007978-3-319-46484-8_2-Figure1-1.png","fratio":2.7016129032258065,"textp":4463143,"paperid":31,"fignums":9,"pagenums":16},],32:[{"size":33900,"src":"./images/10.1007978-3-319-46484-8_4-Figure1-1.png","figureid":"10.1007978-3-319-46484-8_4-Figure1-1.png","fratio":3.39,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},{"size":49155,"src":"./images/10.1007978-3-319-46484-8_4-Figure3-1.png","figureid":"10.1007978-3-319-46484-8_4-Figure3-1.png","fratio":2.3379310344827586,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},{"size":47196,"src":"./images/10.1007978-3-319-46484-8_4-Figure6-1.png","figureid":"10.1007978-3-319-46484-8_4-Figure6-1.png","fratio":2.4782608695652173,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},{"size":31613,"src":"./images/10.1007978-3-319-46484-8_4-Table2-1.png","figureid":"10.1007978-3-319-46484-8_4-Table2-1.png","fratio":3.099009900990099,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},{"size":49248,"src":"./images/10.1007978-3-319-46484-8_4-Figure5-1.png","figureid":"10.1007978-3-319-46484-8_4-Figure5-1.png","fratio":2.375,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},{"size":20032,"src":"./images/10.1007978-3-319-46484-8_4-Table1-1.png","figureid":"10.1007978-3-319-46484-8_4-Table1-1.png","fratio":4.890625,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},{"size":39672,"src":"./images/10.1007978-3-319-46484-8_4-Figure4-1.png","figureid":"10.1007978-3-319-46484-8_4-Figure4-1.png","fratio":2.9482758620689653,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},{"size":20178,"src":"./images/10.1007978-3-319-46484-8_4-Figure2-1.png","figureid":"10.1007978-3-319-46484-8_4-Figure2-1.png","fratio":5.796610169491525,"textp":4463143,"paperid":32,"fignums":8,"pagenums":16},],42:[{"size":58653,"src":"./images/10.1007978-3-319-46493-0_45-Figure3-1.png","figureid":"10.1007978-3-319-46493-0_45-Figure3-1.png","fratio":2.0058479532163744,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":21735,"src":"./images/10.1007978-3-319-46493-0_45-Table2-1.png","figureid":"10.1007978-3-319-46493-0_45-Table2-1.png","fratio":5.476190476190476,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":15048,"src":"./images/10.1007978-3-319-46493-0_45-Table3-1.png","figureid":"10.1007978-3-319-46493-0_45-Table3-1.png","fratio":7.7727272727272725,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":21452,"src":"./images/10.1007978-3-319-46493-0_45-Table1-1.png","figureid":"10.1007978-3-319-46493-0_45-Table1-1.png","fratio":5.580645161290323,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":52397,"src":"./images/10.1007978-3-319-46493-0_45-Table6-1.png","figureid":"10.1007978-3-319-46493-0_45-Table6-1.png","fratio":2.2980132450331126,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":18096,"src":"./images/10.1007978-3-319-46493-0_45-Table4-1.png","figureid":"10.1007978-3-319-46493-0_45-Table4-1.png","fratio":6.6923076923076925,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":13572,"src":"./images/10.1007978-3-319-46493-0_45-Table5-1.png","figureid":"10.1007978-3-319-46493-0_45-Table5-1.png","fratio":8.923076923076923,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":43218,"src":"./images/10.1007978-3-319-46493-0_45-Figure1-1.png","figureid":"10.1007978-3-319-46493-0_45-Figure1-1.png","fratio":2.7222222222222223,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":77490,"src":"./images/10.1007978-3-319-46493-0_45-Figure5-1.png","figureid":"10.1007978-3-319-46493-0_45-Figure5-1.png","fratio":1.0629629629629629,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":26040,"src":"./images/10.1007978-3-319-46493-0_45-Figure4-1.png","figureid":"10.1007978-3-319-46493-0_45-Figure4-1.png","fratio":3.6904761904761907,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},{"size":40870,"src":"./images/10.1007978-3-319-46493-0_45-Figure2-1.png","figureid":"10.1007978-3-319-46493-0_45-Figure2-1.png","fratio":2.7459016393442623,"textp":4463143,"paperid":42,"fignums":11,"pagenums":16},],20:[{"size":13968,"src":"./images/10.1109CVPR.2007.383007-Figure1-1.png","figureid":"10.1109CVPR.2007.383007-Figure1-1.png","fratio":2.6944444444444446,"textp":4463143,"paperid":20,"fignums":4,"pagenums":6},{"size":180747,"src":"./images/10.1109CVPR.2007.383007-Figure4-1.png","figureid":"10.1109CVPR.2007.383007-Figure4-1.png","fratio":1.1353383458646618,"textp":4463143,"paperid":20,"fignums":4,"pagenums":6},{"size":116380,"src":"./images/10.1109CVPR.2007.383007-Figure3-1.png","figureid":"10.1109CVPR.2007.383007-Figure3-1.png","fratio":0.4158790170132325,"textp":4463143,"paperid":20,"fignums":4,"pagenums":6},{"size":30096,"src":"./images/10.1109CVPR.2007.383007-Figure2-1.png","figureid":"10.1109CVPR.2007.383007-Figure2-1.png","fratio":1.3026315789473684,"textp":4463143,"paperid":20,"fignums":4,"pagenums":6},],0:[{"size":30378,"src":"./images/10.1109CVPR.2007.383263-Figure1-1.png","figureid":"10.1109CVPR.2007.383263-Figure1-1.png","fratio":2.040983606557377,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":67798,"src":"./images/10.1109CVPR.2007.383263-Figure2-1.png","figureid":"10.1109CVPR.2007.383263-Figure2-1.png","fratio":0.7009646302250804,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":6513,"src":"./images/10.1109CVPR.2007.383263-Table1-1.png","figureid":"10.1109CVPR.2007.383263-Table1-1.png","fratio":4.282051282051282,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":7800,"src":"./images/10.1109CVPR.2007.383263-Table2-1.png","figureid":"10.1109CVPR.2007.383263-Table2-1.png","fratio":4.875,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":28000,"src":"./images/10.1109CVPR.2007.383263-Figure6-1.png","figureid":"10.1109CVPR.2007.383263-Figure6-1.png","fratio":1.792,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":10075,"src":"./images/10.1109CVPR.2007.383263-Figure3-1.png","figureid":"10.1109CVPR.2007.383263-Figure3-1.png","fratio":2.3846153846153846,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":22140,"src":"./images/10.1109CVPR.2007.383263-Figure8-1.png","figureid":"10.1109CVPR.2007.383263-Figure8-1.png","fratio":1.8981481481481481,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":24150,"src":"./images/10.1109CVPR.2007.383263-Figure7-1.png","figureid":"10.1109CVPR.2007.383263-Figure7-1.png","fratio":1.826086956521739,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":37584,"src":"./images/10.1109CVPR.2007.383263-Figure4-1.png","figureid":"10.1109CVPR.2007.383263-Figure4-1.png","fratio":1.4320987654320987,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},{"size":23800,"src":"./images/10.1109CVPR.2007.383263-Figure5-1.png","figureid":"10.1109CVPR.2007.383263-Figure5-1.png","fratio":0.8235294117647058,"textp":4463143,"paperid":0,"fignums":10,"pagenums":8},],1:[{"size":57120,"src":"./images/10.1109CVPR.2013.345-Figure1-1.png","figureid":"10.1109CVPR.2013.345-Figure1-1.png","fratio":0.9916666666666667,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":35224,"src":"./images/10.1109CVPR.2013.345-Table2-1.png","figureid":"10.1109CVPR.2013.345-Table2-1.png","fratio":1.6081081081081081,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":19521,"src":"./images/10.1109CVPR.2013.345-Table4-1.png","figureid":"10.1109CVPR.2013.345-Table4-1.png","fratio":2.9753086419753085,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":22134,"src":"./images/10.1109CVPR.2013.345-Table3-1.png","figureid":"10.1109CVPR.2013.345-Table3-1.png","fratio":2.5591397849462365,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":82935,"src":"./images/10.1109CVPR.2013.345-Figure6-1.png","figureid":"10.1109CVPR.2013.345-Figure6-1.png","fratio":2.8362573099415203,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":99102,"src":"./images/10.1109CVPR.2013.345-Figure2-1.png","figureid":"10.1109CVPR.2013.345-Figure2-1.png","fratio":2.5025125628140703,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":11092,"src":"./images/10.1109CVPR.2013.345-Table5-1.png","figureid":"10.1109CVPR.2013.345-Table5-1.png","fratio":5.0212765957446805,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":20648,"src":"./images/10.1109CVPR.2013.345-Figure7-1.png","figureid":"10.1109CVPR.2013.345-Figure7-1.png","fratio":2.606741573033708,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":18557,"src":"./images/10.1109CVPR.2013.345-Table1-1.png","figureid":"10.1109CVPR.2013.345-Table1-1.png","fratio":3.1298701298701297,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":17925,"src":"./images/10.1109CVPR.2013.345-Figure3-1.png","figureid":"10.1109CVPR.2013.345-Figure3-1.png","fratio":3.1866666666666665,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":92442,"src":"./images/10.1109CVPR.2013.345-Figure4-1.png","figureid":"10.1109CVPR.2013.345-Figure4-1.png","fratio":2.672043010752688,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},{"size":9600,"src":"./images/10.1109CVPR.2013.345-Figure5-1.png","figureid":"10.1109CVPR.2013.345-Figure5-1.png","fratio":3.84,"textp":4463143,"paperid":1,"fignums":12,"pagenums":8},],2:[{"size":16698,"src":"./images/10.1109CVPR.2014.83-Table1-1.png","figureid":"10.1109CVPR.2014.83-Table1-1.png","fratio":3.5072463768115942,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},{"size":13862,"src":"./images/10.1109CVPR.2014.83-Table2-1.png","figureid":"10.1109CVPR.2014.83-Table2-1.png","fratio":4.120689655172414,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},{"size":19747,"src":"./images/10.1109CVPR.2014.83-Figure5-1.png","figureid":"10.1109CVPR.2014.83-Figure5-1.png","fratio":2.3846153846153846,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},{"size":36386,"src":"./images/10.1109CVPR.2014.83-Figure2-1.png","figureid":"10.1109CVPR.2014.83-Figure2-1.png","fratio":1.4037267080745341,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},{"size":18400,"src":"./images/10.1109CVPR.2014.83-Table3-1.png","figureid":"10.1109CVPR.2014.83-Table3-1.png","fratio":2.875,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},{"size":26740,"src":"./images/10.1109CVPR.2014.83-Figure6-1.png","figureid":"10.1109CVPR.2014.83-Figure6-1.png","fratio":1.3642857142857143,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},{"size":17940,"src":"./images/10.1109CVPR.2014.83-Figure3-1.png","figureid":"10.1109CVPR.2014.83-Figure3-1.png","fratio":1.0615384615384615,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},{"size":70119,"src":"./images/10.1109CVPR.2014.83-Figure4-1.png","figureid":"10.1109CVPR.2014.83-Figure4-1.png","fratio":3.2448979591836733,"textp":4463143,"paperid":2,"fignums":8,"pagenums":8},],3:[{"size":32109,"src":"./images/10.1109CVPR.2015.7299059-Figure1-1.png","figureid":"10.1109CVPR.2015.7299059-Figure1-1.png","fratio":1.6618705035971224,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},{"size":19805,"src":"./images/10.1109CVPR.2015.7299059-Figure3-1.png","figureid":"10.1109CVPR.2015.7299059-Figure3-1.png","fratio":2.7411764705882353,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},{"size":42016,"src":"./images/10.1109CVPR.2015.7299059-Table3-1.png","figureid":"10.1109CVPR.2015.7299059-Table3-1.png","fratio":0.9711538461538461,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},{"size":94656,"src":"./images/10.1109CVPR.2015.7299059-Figure2-1.png","figureid":"10.1109CVPR.2015.7299059-Figure2-1.png","fratio":2.5677083333333335,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},{"size":26796,"src":"./images/10.1109CVPR.2015.7299059-Table4-1.png","figureid":"10.1109CVPR.2015.7299059-Table4-1.png","fratio":1.9913793103448276,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},{"size":18392,"src":"./images/10.1109CVPR.2015.7299059-Table2-1.png","figureid":"10.1109CVPR.2015.7299059-Table2-1.png","fratio":12.736842105263158,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},{"size":76588,"src":"./images/10.1109CVPR.2015.7299059-Figure4-1.png","figureid":"10.1109CVPR.2015.7299059-Figure4-1.png","fratio":2.847560975609756,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},{"size":36432,"src":"./images/10.1109CVPR.2015.7299059-Table1-1.png","figureid":"10.1109CVPR.2015.7299059-Table1-1.png","fratio":7.027777777777778,"textp":4463143,"paperid":3,"fignums":8,"pagenums":10},],4:[{"size":60298,"src":"./images/10.1109CVPR.2016.219-Table1-1.png","figureid":"10.1109CVPR.2016.219-Table1-1.png","fratio":2.828767123287671,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":5994,"src":"./images/10.1109CVPR.2016.219-Table2-1.png","figureid":"10.1109CVPR.2016.219-Table2-1.png","fratio":8.222222222222221,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":85656,"src":"./images/10.1109CVPR.2016.219-Figure5-1.png","figureid":"10.1109CVPR.2016.219-Figure5-1.png","fratio":2.895348837209302,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":24128,"src":"./images/10.1109CVPR.2016.219-Figure2-1.png","figureid":"10.1109CVPR.2016.219-Figure2-1.png","fratio":1.793103448275862,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":9664,"src":"./images/10.1109CVPR.2016.219-Table4-1.png","figureid":"10.1109CVPR.2016.219-Table4-1.png","fratio":2.359375,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":24080,"src":"./images/10.1109CVPR.2016.219-Table3-1.png","figureid":"10.1109CVPR.2016.219-Table3-1.png","fratio":1.9196428571428572,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":102144,"src":"./images/10.1109CVPR.2016.219-Figure6-1.png","figureid":"10.1109CVPR.2016.219-Figure6-1.png","fratio":1.9649122807017543,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":98340,"src":"./images/10.1109CVPR.2016.219-Figure3-1.png","figureid":"10.1109CVPR.2016.219-Figure3-1.png","fratio":2.0318181818181817,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},{"size":62992,"src":"./images/10.1109CVPR.2016.219-Figure4-1.png","figureid":"10.1109CVPR.2016.219-Figure4-1.png","fratio":3.905511811023622,"textp":4463143,"paperid":4,"fignums":9,"pagenums":9},],5:[{"size":20774,"src":"./images/10.1109CVPR.2016.296-Table1-1.png","figureid":"10.1109CVPR.2016.296-Table1-1.png","fratio":2.351063829787234,"textp":4463143,"paperid":5,"fignums":7,"pagenums":10},{"size":21385,"src":"./images/10.1109CVPR.2016.296-Figure4-1.png","figureid":"10.1109CVPR.2016.296-Figure4-1.png","fratio":2.5824175824175826,"textp":4463143,"paperid":5,"fignums":7,"pagenums":10},{"size":67068,"src":"./images/10.1109CVPR.2016.296-Figure5-1.png","figureid":"10.1109CVPR.2016.296-Figure5-1.png","fratio":3.5217391304347827,"textp":4463143,"paperid":5,"fignums":7,"pagenums":10},{"size":104940,"src":"./images/10.1109CVPR.2016.296-Figure6-1.png","figureid":"10.1109CVPR.2016.296-Figure6-1.png","fratio":2.168181818181818,"textp":4463143,"paperid":5,"fignums":7,"pagenums":10},{"size":78400,"src":"./images/10.1109CVPR.2016.296-Figure2-1.png","figureid":"10.1109CVPR.2016.296-Figure2-1.png","fratio":3.0625,"textp":4463143,"paperid":5,"fignums":7,"pagenums":10},{"size":45942,"src":"./images/10.1109CVPR.2016.296-Table2-1.png","figureid":"10.1109CVPR.2016.296-Table2-1.png","fratio":5.311827956989247,"textp":4463143,"paperid":5,"fignums":7,"pagenums":10},{"size":42054,"src":"./images/10.1109CVPR.2016.296-Figure3-1.png","figureid":"10.1109CVPR.2016.296-Figure3-1.png","fratio":5.686046511627907,"textp":4463143,"paperid":5,"fignums":7,"pagenums":10},],7:[{"size":14322,"src":"./images/10.1109CVPR.2016.297-Table1-1.png","figureid":"10.1109CVPR.2016.297-Table1-1.png","fratio":2.4155844155844157,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":13156,"src":"./images/10.1109CVPR.2016.297-Table4-1.png","figureid":"10.1109CVPR.2016.297-Table4-1.png","fratio":4.865384615384615,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":15049,"src":"./images/10.1109CVPR.2016.297-Table2-1.png","figureid":"10.1109CVPR.2016.297-Table2-1.png","fratio":1.4752475247524752,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":8904,"src":"./images/10.1109CVPR.2016.297-Table3-1.png","figureid":"10.1109CVPR.2016.297-Table3-1.png","fratio":3.169811320754717,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":9307,"src":"./images/10.1109CVPR.2016.297-Table6-1.png","figureid":"10.1109CVPR.2016.297-Table6-1.png","fratio":5.536585365853658,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":21627,"src":"./images/10.1109CVPR.2016.297-Table7-1.png","figureid":"10.1109CVPR.2016.297-Table7-1.png","fratio":2.730337078651685,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":11804,"src":"./images/10.1109CVPR.2016.297-Table5-1.png","figureid":"10.1109CVPR.2016.297-Table5-1.png","fratio":4.365384615384615,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":12780,"src":"./images/10.1109CVPR.2016.297-Figure4-1.png","figureid":"10.1109CVPR.2016.297-Figure4-1.png","fratio":3.55,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":18326,"src":"./images/10.1109CVPR.2016.297-Table8-1.png","figureid":"10.1109CVPR.2016.297-Table8-1.png","fratio":3.090909090909091,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":101885,"src":"./images/10.1109CVPR.2016.297-Figure2-1.png","figureid":"10.1109CVPR.2016.297-Figure2-1.png","fratio":2.424390243902439,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},{"size":54438,"src":"./images/10.1109CVPR.2016.297-Figure3-1.png","figureid":"10.1109CVPR.2016.297-Figure3-1.png","fratio":3.2713178294573644,"textp":4463143,"paperid":7,"fignums":11,"pagenums":9},],6:[{"size":33319,"src":"./images/10.1109CVPR.2016.529-Table1-1.png","figureid":"10.1109CVPR.2016.529-Table1-1.png","fratio":1.6293706293706294,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":16647,"src":"./images/10.1109CVPR.2016.529-Table2-1.png","figureid":"10.1109CVPR.2016.529-Table2-1.png","fratio":1.924731182795699,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":39150,"src":"./images/10.1109CVPR.2016.529-Figure4-1.png","figureid":"10.1109CVPR.2016.529-Figure4-1.png","fratio":1.293103448275862,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":45024,"src":"./images/10.1109CVPR.2016.529-Figure2-1.png","figureid":"10.1109CVPR.2016.529-Figure2-1.png","fratio":0.8973214285714286,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":14596,"src":"./images/10.1109CVPR.2016.529-Table4-1.png","figureid":"10.1109CVPR.2016.529-Table4-1.png","fratio":2.1707317073170733,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":32915,"src":"./images/10.1109CVPR.2016.529-Figure5-1.png","figureid":"10.1109CVPR.2016.529-Figure5-1.png","fratio":1.5655172413793104,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":19646,"src":"./images/10.1109CVPR.2016.529-Table3-1.png","figureid":"10.1109CVPR.2016.529-Table3-1.png","fratio":2.223404255319149,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":28684,"src":"./images/10.1109CVPR.2016.529-Figure6-1.png","figureid":"10.1109CVPR.2016.529-Figure6-1.png","fratio":1.4225352112676057,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":107646,"src":"./images/10.1109CVPR.2016.529-Figure3-1.png","figureid":"10.1109CVPR.2016.529-Figure3-1.png","fratio":1.9828326180257512,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},{"size":20504,"src":"./images/10.1109CVPR.2016.529-Table5-1.png","figureid":"10.1109CVPR.2016.529-Table5-1.png","fratio":2.647727272727273,"textp":4463143,"paperid":6,"fignums":10,"pagenums":9},],18:[{"size":23868,"src":"./images/10.1109CVPR.2018.00070-Figure1-1.png","figureid":"10.1109CVPR.2018.00070-Figure1-1.png","fratio":2.2941176470588234,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":133424,"src":"./images/10.1109CVPR.2018.00070-Figure5-1.png","figureid":"10.1109CVPR.2018.00070-Figure5-1.png","fratio":1.8438661710037174,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":59280,"src":"./images/10.1109CVPR.2018.00070-Figure2-1.png","figureid":"10.1109CVPR.2018.00070-Figure2-1.png","fratio":4.116666666666666,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":20468,"src":"./images/10.1109CVPR.2018.00070-Figure8-1.png","figureid":"10.1109CVPR.2018.00070-Figure8-1.png","fratio":2.7674418604651163,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":19716,"src":"./images/10.1109CVPR.2018.00070-Figure6-1.png","figureid":"10.1109CVPR.2018.00070-Figure6-1.png","fratio":2.2795698924731185,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":17302,"src":"./images/10.1109CVPR.2018.00070-Figure7-1.png","figureid":"10.1109CVPR.2018.00070-Figure7-1.png","fratio":2.573170731707317,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":21420,"src":"./images/10.1109CVPR.2018.00070-Figure9-1.png","figureid":"10.1109CVPR.2018.00070-Figure9-1.png","fratio":2.6444444444444444,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":47637,"src":"./images/10.1109CVPR.2018.00070-Figure11-1.png","figureid":"10.1109CVPR.2018.00070-Figure11-1.png","fratio":1.1791044776119404,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":34365,"src":"./images/10.1109CVPR.2018.00070-Figure10-1.png","figureid":"10.1109CVPR.2018.00070-Figure10-1.png","fratio":1.6344827586206896,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":74745,"src":"./images/10.1109CVPR.2018.00070-Figure3-1.png","figureid":"10.1109CVPR.2018.00070-Figure3-1.png","fratio":3.2781456953642385,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},{"size":42244,"src":"./images/10.1109CVPR.2018.00070-Figure4-1.png","figureid":"10.1109CVPR.2018.00070-Figure4-1.png","fratio":1.318435754189944,"textp":4463143,"paperid":18,"fignums":11,"pagenums":10},],19:[{"size":8052,"src":"./images/10.1109CVPR.2018.00259-Table2-1.png","figureid":"10.1109CVPR.2018.00259-Table2-1.png","fratio":7.393939393939394,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":13740,"src":"./images/10.1109CVPR.2018.00259-Figure4-1.png","figureid":"10.1109CVPR.2018.00259-Figure4-1.png","fratio":3.816666666666667,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":15314,"src":"./images/10.1109CVPR.2018.00259-Table3-1.png","figureid":"10.1109CVPR.2018.00259-Table3-1.png","fratio":3.9838709677419355,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":42180,"src":"./images/10.1109CVPR.2018.00259-Figure1-1.png","figureid":"10.1109CVPR.2018.00259-Figure1-1.png","fratio":1.2324324324324325,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":106952,"src":"./images/10.1109CVPR.2018.00259-Figure5-1.png","figureid":"10.1109CVPR.2018.00259-Figure5-1.png","fratio":1.9870689655172413,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":24514,"src":"./images/10.1109CVPR.2018.00259-Figure6-1.png","figureid":"10.1109CVPR.2018.00259-Figure6-1.png","fratio":2.3106796116504853,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":40740,"src":"./images/10.1109CVPR.2018.00259-Figure7-1.png","figureid":"10.1109CVPR.2018.00259-Figure7-1.png","fratio":1.0824742268041236,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":41132,"src":"./images/10.1109CVPR.2018.00259-Figure2-1.png","figureid":"10.1109CVPR.2018.00259-Figure2-1.png","fratio":1.2417582417582418,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":15314,"src":"./images/10.1109CVPR.2018.00259-Table6-1.png","figureid":"10.1109CVPR.2018.00259-Table6-1.png","fratio":3.9838709677419355,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":18881,"src":"./images/10.1109CVPR.2018.00259-Table4-1.png","figureid":"10.1109CVPR.2018.00259-Table4-1.png","fratio":3.0253164556962027,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":10621,"src":"./images/10.1109CVPR.2018.00259-Table7-1.png","figureid":"10.1109CVPR.2018.00259-Table7-1.png","fratio":5.744186046511628,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":12532,"src":"./images/10.1109CVPR.2018.00259-Table5-1.png","figureid":"10.1109CVPR.2018.00259-Table5-1.png","fratio":4.634615384615385,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":21669,"src":"./images/10.1109CVPR.2018.00259-Table1-1.png","figureid":"10.1109CVPR.2018.00259-Table1-1.png","fratio":2.5053763440860215,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},{"size":47412,"src":"./images/10.1109CVPR.2018.00259-Figure3-1.png","figureid":"10.1109CVPR.2018.00259-Figure3-1.png","fratio":4.064814814814815,"textp":4463143,"paperid":19,"fignums":14,"pagenums":10},],9:[{"size":14848,"src":"./images/10.1109CVPR.2018.00527-Table1-1.png","figureid":"10.1109CVPR.2018.00527-Table1-1.png","fratio":3.625,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":11322,"src":"./images/10.1109CVPR.2018.00527-Table2-1.png","figureid":"10.1109CVPR.2018.00527-Table2-1.png","fratio":4.352941176470588,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":65412,"src":"./images/10.1109CVPR.2018.00527-Figure4-1.png","figureid":"10.1109CVPR.2018.00527-Figure4-1.png","fratio":0.6550632911392406,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":132404,"src":"./images/10.1109CVPR.2018.00527-Figure1-1.png","figureid":"10.1109CVPR.2018.00527-Figure1-1.png","fratio":1.3259493670886076,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":17400,"src":"./images/10.1109CVPR.2018.00527-Table4-1.png","figureid":"10.1109CVPR.2018.00527-Table4-1.png","fratio":2.2988505747126435,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":8580,"src":"./images/10.1109CVPR.2018.00527-Table3-1.png","figureid":"10.1109CVPR.2018.00527-Table3-1.png","fratio":5.641025641025641,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":38560,"src":"./images/10.1109CVPR.2018.00527-Table5-1.png","figureid":"10.1109CVPR.2018.00527-Table5-1.png","fratio":1.50625,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":42840,"src":"./images/10.1109CVPR.2018.00527-Figure2-1.png","figureid":"10.1109CVPR.2018.00527-Figure2-1.png","fratio":4.117647058823529,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":62463,"src":"./images/10.1109CVPR.2018.00527-Figure5-1.png","figureid":"10.1109CVPR.2018.00527-Figure5-1.png","fratio":3.141843971631206,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},{"size":65985,"src":"./images/10.1109CVPR.2018.00527-Figure3-1.png","figureid":"10.1109CVPR.2018.00527-Figure3-1.png","fratio":2.610062893081761,"textp":4463143,"paperid":9,"fignums":10,"pagenums":10},],8:[{"size":40290,"src":"./images/10.1109CVPR.2018.00557-Figure1-1.png","figureid":"10.1109CVPR.2018.00557-Figure1-1.png","fratio":1.3941176470588235,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":56376,"src":"./images/10.1109CVPR.2018.00557-Table1-1.png","figureid":"10.1109CVPR.2018.00557-Table1-1.png","fratio":4.189655172413793,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":26352,"src":"./images/10.1109CVPR.2018.00557-Table2-1.png","figureid":"10.1109CVPR.2018.00557-Table2-1.png","fratio":9.037037037037036,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":40200,"src":"./images/10.1109CVPR.2018.00557-Figure5-1.png","figureid":"10.1109CVPR.2018.00557-Figure5-1.png","fratio":4.02,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":103944,"src":"./images/10.1109CVPR.2018.00557-Figure6-1.png","figureid":"10.1109CVPR.2018.00557-Figure6-1.png","fratio":2.291079812206573,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":21545,"src":"./images/10.1109CVPR.2018.00557-Figure2-1.png","figureid":"10.1109CVPR.2018.00557-Figure2-1.png","fratio":1.1151079136690647,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":26912,"src":"./images/10.1109CVPR.2018.00557-Table4-1.png","figureid":"10.1109CVPR.2018.00557-Table4-1.png","fratio":2.0,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":26768,"src":"./images/10.1109CVPR.2018.00557-Table3-1.png","figureid":"10.1109CVPR.2018.00557-Table3-1.png","fratio":8.535714285714286,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":87560,"src":"./images/10.1109CVPR.2018.00557-Figure7-1.png","figureid":"10.1109CVPR.2018.00557-Figure7-1.png","fratio":0.5527638190954773,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":28875,"src":"./images/10.1109CVPR.2018.00557-Figure4-1.png","figureid":"10.1109CVPR.2018.00557-Figure4-1.png","fratio":1.848,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},{"size":21777,"src":"./images/10.1109CVPR.2018.00557-Figure3-1.png","figureid":"10.1109CVPR.2018.00557-Figure3-1.png","fratio":1.5378151260504203,"textp":4463143,"paperid":8,"fignums":11,"pagenums":9},],10:[{"size":39216,"src":"./images/10.1109CVPR.2018.00595-Figure1-1.png","figureid":"10.1109CVPR.2018.00595-Figure1-1.png","fratio":1.3255813953488371,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":75768,"src":"./images/10.1109CVPR.2018.00595-Table2-1.png","figureid":"10.1109CVPR.2018.00595-Table2-1.png","fratio":3.1948051948051948,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":22736,"src":"./images/10.1109CVPR.2018.00595-Table3-1.png","figureid":"10.1109CVPR.2018.00595-Table3-1.png","fratio":2.36734693877551,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":35136,"src":"./images/10.1109CVPR.2018.00595-Figure2-1.png","figureid":"10.1109CVPR.2018.00595-Figure2-1.png","fratio":6.777777777777778,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":67248,"src":"./images/10.1109CVPR.2018.00595-Table4-1.png","figureid":"10.1109CVPR.2018.00595-Table4-1.png","fratio":3.2430555555555554,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":70226,"src":"./images/10.1109CVPR.2018.00595-Figure5-1.png","figureid":"10.1109CVPR.2018.00595-Figure5-1.png","fratio":3.2945205479452055,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":26455,"src":"./images/10.1109CVPR.2018.00595-Table1-1.png","figureid":"10.1109CVPR.2018.00595-Table1-1.png","fratio":1.2937062937062938,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":17020,"src":"./images/10.1109CVPR.2018.00595-Figure3-1.png","figureid":"10.1109CVPR.2018.00595-Figure3-1.png","fratio":3.108108108108108,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":69524,"src":"./images/10.1109CVPR.2018.00595-Figure6-1.png","figureid":"10.1109CVPR.2018.00595-Figure6-1.png","fratio":2.098901098901099,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":17222,"src":"./images/10.1109CVPR.2018.00595-Table5-1.png","figureid":"10.1109CVPR.2018.00595-Table5-1.png","fratio":2.759493670886076,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},{"size":22866,"src":"./images/10.1109CVPR.2018.00595-Figure4-1.png","figureid":"10.1109CVPR.2018.00595-Figure4-1.png","fratio":2.1553398058252426,"textp":4463143,"paperid":10,"fignums":11,"pagenums":10},],21:[{"size":26096,"src":"./images/10.1109ICCV.2013.333-Figure4-1.png","figureid":"10.1109ICCV.2013.333-Figure4-1.png","fratio":2.080357142857143,"textp":4463143,"paperid":21,"fignums":7,"pagenums":8},{"size":16863,"src":"./images/10.1109ICCV.2013.333-Table1-1.png","figureid":"10.1109ICCV.2013.333-Table1-1.png","fratio":3.164383561643836,"textp":4463143,"paperid":21,"fignums":7,"pagenums":8},{"size":20381,"src":"./images/10.1109ICCV.2013.333-Table2-1.png","figureid":"10.1109ICCV.2013.333-Table2-1.png","fratio":2.5730337078651684,"textp":4463143,"paperid":21,"fignums":7,"pagenums":8},{"size":101388,"src":"./images/10.1109ICCV.2013.333-Figure5-1.png","figureid":"10.1109ICCV.2013.333-Figure5-1.png","fratio":2.4362745098039214,"textp":4463143,"paperid":21,"fignums":7,"pagenums":8},{"size":24582,"src":"./images/10.1109ICCV.2013.333-Table3-1.png","figureid":"10.1109ICCV.2013.333-Table3-1.png","fratio":2.3627450980392157,"textp":4463143,"paperid":21,"fignums":7,"pagenums":8},{"size":85158,"src":"./images/10.1109ICCV.2013.333-Figure2-1.png","figureid":"10.1109ICCV.2013.333-Figure2-1.png","fratio":2.912280701754386,"textp":4463143,"paperid":21,"fignums":7,"pagenums":8},{"size":21510,"src":"./images/10.1109ICCV.2013.333-Figure3-1.png","figureid":"10.1109ICCV.2013.333-Figure3-1.png","fratio":2.6555555555555554,"textp":4463143,"paperid":21,"fignums":7,"pagenums":8},],24:[{"size":17550,"src":"./images/10.1109ICCV.2017.331-Table1-1.png","figureid":"10.1109ICCV.2017.331-Table1-1.png","fratio":3.12,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":19688,"src":"./images/10.1109ICCV.2017.331-Table2-1.png","figureid":"10.1109ICCV.2017.331-Table2-1.png","fratio":2.3260869565217392,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":88263,"src":"./images/10.1109ICCV.2017.331-Figure7-1.png","figureid":"10.1109ICCV.2017.331-Figure7-1.png","fratio":2.4708994708994707,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":86395,"src":"./images/10.1109ICCV.2017.331-Figure2-1.png","figureid":"10.1109ICCV.2017.331-Figure2-1.png","fratio":2.5243243243243243,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":18146,"src":"./images/10.1109ICCV.2017.331-Table4-1.png","figureid":"10.1109ICCV.2017.331-Table4-1.png","fratio":2.453488372093023,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":87474,"src":"./images/10.1109ICCV.2017.331-Table3-1.png","figureid":"10.1109ICCV.2017.331-Table3-1.png","fratio":2.612021857923497,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":31524,"src":"./images/10.1109ICCV.2017.331-Figure3-1.png","figureid":"10.1109ICCV.2017.331-Figure3-1.png","fratio":1.5633802816901408,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":27086,"src":"./images/10.1109ICCV.2017.331-Figure4-1.png","figureid":"10.1109ICCV.2017.331-Figure4-1.png","fratio":8.051724137931034,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":22300,"src":"./images/10.1109ICCV.2017.331-Figure5-1.png","figureid":"10.1109ICCV.2017.331-Figure5-1.png","fratio":2.23,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},{"size":31666,"src":"./images/10.1109ICCV.2017.331-Figure6-1.png","figureid":"10.1109ICCV.2017.331-Figure6-1.png","fratio":1.5704225352112675,"textp":4463143,"paperid":24,"fignums":10,"pagenums":9},],25:[{"size":64347,"src":"./images/10.1109ICCV.2017.344-Figure1-1.png","figureid":"10.1109ICCV.2017.344-Figure1-1.png","fratio":0.9026217228464419,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":26400,"src":"./images/10.1109ICCV.2017.344-Figure4-1.png","figureid":"10.1109ICCV.2017.344-Figure4-1.png","fratio":2.1818181818181817,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":16800,"src":"./images/10.1109ICCV.2017.344-Figure5-1.png","figureid":"10.1109ICCV.2017.344-Figure5-1.png","fratio":3.4285714285714284,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":26510,"src":"./images/10.1109ICCV.2017.344-Figure6-1.png","figureid":"10.1109ICCV.2017.344-Figure6-1.png","fratio":2.190909090909091,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":13260,"src":"./images/10.1109ICCV.2017.344-Table1-1.png","figureid":"10.1109ICCV.2017.344-Table1-1.png","fratio":3.1384615384615384,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":14916,"src":"./images/10.1109ICCV.2017.344-Table2-1.png","figureid":"10.1109ICCV.2017.344-Table2-1.png","fratio":3.4242424242424243,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":103412,"src":"./images/10.1109ICCV.2017.344-Figure7-1.png","figureid":"10.1109ICCV.2017.344-Figure7-1.png","fratio":2.436893203883495,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":90862,"src":"./images/10.1109ICCV.2017.344-Figure2-1.png","figureid":"10.1109ICCV.2017.344-Figure2-1.png","fratio":2.773480662983425,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":122990,"src":"./images/10.1109ICCV.2017.344-Figure8-1.png","figureid":"10.1109ICCV.2017.344-Figure8-1.png","fratio":2.048979591836735,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":72355,"src":"./images/10.1109ICCV.2017.344-Figure9-1.png","figureid":"10.1109ICCV.2017.344-Figure9-1.png","fratio":3.4413793103448276,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},{"size":95380,"src":"./images/10.1109ICCV.2017.344-Figure3-1.png","figureid":"10.1109ICCV.2017.344-Figure3-1.png","fratio":2.642105263157895,"textp":4463143,"paperid":25,"fignums":11,"pagenums":9},],23:[{"size":14848,"src":"./images/10.1109ICCV.2017.402-Table1-1.png","figureid":"10.1109ICCV.2017.402-Table1-1.png","fratio":3.625,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":11322,"src":"./images/10.1109ICCV.2017.402-Table2-1.png","figureid":"10.1109ICCV.2017.402-Table2-1.png","fratio":4.352941176470588,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":65412,"src":"./images/10.1109ICCV.2017.402-Figure4-1.png","figureid":"10.1109ICCV.2017.402-Figure4-1.png","fratio":0.6550632911392406,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":132404,"src":"./images/10.1109ICCV.2017.402-Figure1-1.png","figureid":"10.1109ICCV.2017.402-Figure1-1.png","fratio":1.3259493670886076,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":17400,"src":"./images/10.1109ICCV.2017.402-Table4-1.png","figureid":"10.1109ICCV.2017.402-Table4-1.png","fratio":2.2988505747126435,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":8580,"src":"./images/10.1109ICCV.2017.402-Table3-1.png","figureid":"10.1109ICCV.2017.402-Table3-1.png","fratio":5.641025641025641,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":38560,"src":"./images/10.1109ICCV.2017.402-Table5-1.png","figureid":"10.1109ICCV.2017.402-Table5-1.png","fratio":1.50625,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":42840,"src":"./images/10.1109ICCV.2017.402-Figure2-1.png","figureid":"10.1109ICCV.2017.402-Figure2-1.png","fratio":4.117647058823529,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":62463,"src":"./images/10.1109ICCV.2017.402-Figure5-1.png","figureid":"10.1109ICCV.2017.402-Figure5-1.png","fratio":3.141843971631206,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},{"size":65985,"src":"./images/10.1109ICCV.2017.402-Figure3-1.png","figureid":"10.1109ICCV.2017.402-Figure3-1.png","fratio":2.610062893081761,"textp":4463143,"paperid":23,"fignums":10,"pagenums":10},],22:[{"size":50640,"src":"./images/10.1109ICCV.2017.578-Figure4-1.png","figureid":"10.1109ICCV.2017.578-Figure4-1.png","fratio":1.1374407582938388,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":47477,"src":"./images/10.1109ICCV.2017.578-Figure1-1.png","figureid":"10.1109ICCV.2017.578-Figure1-1.png","fratio":1.2233502538071066,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":18018,"src":"./images/10.1109ICCV.2017.578-Table1-1.png","figureid":"10.1109ICCV.2017.578-Table1-1.png","fratio":2.9615384615384617,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":16247,"src":"./images/10.1109ICCV.2017.578-Table2-1.png","figureid":"10.1109ICCV.2017.578-Table2-1.png","fratio":2.74025974025974,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":12090,"src":"./images/10.1109ICCV.2017.578-Table3-1.png","figureid":"10.1109ICCV.2017.578-Table3-1.png","fratio":2.8615384615384616,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":40560,"src":"./images/10.1109ICCV.2017.578-Figure2-1.png","figureid":"10.1109ICCV.2017.578-Figure2-1.png","fratio":1.4201183431952662,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":21836,"src":"./images/10.1109ICCV.2017.578-Table6-1.png","figureid":"10.1109ICCV.2017.578-Table6-1.png","fratio":7.773584905660377,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":19780,"src":"./images/10.1109ICCV.2017.578-Table7-1.png","figureid":"10.1109ICCV.2017.578-Table7-1.png","fratio":1.4956521739130435,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":34638,"src":"./images/10.1109ICCV.2017.578-Figure5-1.png","figureid":"10.1109ICCV.2017.578-Figure5-1.png","fratio":7.27536231884058,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":43877,"src":"./images/10.1109ICCV.2017.578-Table4-1.png","figureid":"10.1109ICCV.2017.578-Table4-1.png","fratio":5.53932584269663,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":70782,"src":"./images/10.1109ICCV.2017.578-Figure3-1.png","figureid":"10.1109ICCV.2017.578-Figure3-1.png","fratio":3.5602836879432624,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},{"size":105327,"src":"./images/10.1109ICCV.2017.578-Table5-1.png","figureid":"10.1109ICCV.2017.578-Table5-1.png","fratio":1.6987951807228916,"textp":4463143,"paperid":22,"fignums":12,"pagenums":10},],11:[{"size":14742,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Figure1-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Figure1-1.png","fratio":3.7142857142857144,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":14620,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table4-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table4-1.png","fratio":3.161764705882353,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":42742,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table2-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table2-1.png","fratio":5.77906976744186,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":17200,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table3-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table3-1.png","fratio":2.6875,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":16555,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table6-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table6-1.png","fratio":2.792207792207792,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":29845,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Figure4-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Figure4-1.png","fratio":1.8503937007874016,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":20425,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table5-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table5-1.png","fratio":2.263157894736842,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":76105,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Figure2-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Figure2-1.png","fratio":3.167741935483871,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":18326,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table7-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table7-1.png","fratio":3.090909090909091,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":18403,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table8-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table8-1.png","fratio":3.103896103896104,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":42398,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Figure3-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Figure3-1.png","fratio":5.732558139534884,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},{"size":10472,"src":"./images/Adaptive Pyramid Context Network for Semantic Segmentation-Table1-1.png","figureid":"Adaptive Pyramid Context Network for Semantic Segmentation-Table1-1.png","fratio":3.3392857142857144,"textp":4463143,"paperid":11,"fignums":12,"pagenums":10},],14:[{"size":44932,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure1-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure1-1.png","fratio":1.2712765957446808,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":66693,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Table1-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Table1-1.png","fratio":3.354609929078014,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":94940,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Table2-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Table2-1.png","fratio":2.3267326732673266,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":28438,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Table3-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Table3-1.png","fratio":2.042372881355932,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":63726,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure6-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure6-1.png","fratio":3.8294573643410854,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":49952,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure2-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure2-1.png","fratio":1.0044843049327354,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":58320,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure8-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure8-1.png","fratio":4.05,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":34894,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure10-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure10-1.png","fratio":1.6369863013698631,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":13790,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure9-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure9-1.png","fratio":2.8142857142857145,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":18841,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure3-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure3-1.png","fratio":2.7349397590361444,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":51584,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure4-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure4-1.png","fratio":4.769230769230769,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},{"size":53640,"src":"./images/Blind Super-Resolution With Iterative Kernel Correction-Figure5-1.png","figureid":"Blind Super-Resolution With Iterative Kernel Correction-Figure5-1.png","fratio":3.725,"textp":4463143,"paperid":14,"fignums":12,"pagenums":10},],16:[{"size":99495,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure5-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure5-1.png","fratio":2.462686567164179,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":35611,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure11-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure11-1.png","fratio":1.604026845637584,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":121272,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure16-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure16-1.png","fratio":1.971774193548387,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":41990,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure17-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure17-1.png","fratio":5.811764705882353,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":147258,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure7-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure7-1.png","fratio":1.603960396039604,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":36176,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Table1-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Table1-1.png","fratio":1.5657894736842106,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":119196,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure14-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure14-1.png","fratio":1.7906976744186047,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":22572,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure15-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure15-1.png","fratio":2.303030303030303,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":112800,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure8-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure8-1.png","fratio":1.9583333333333333,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":25833,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure10-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure10-1.png","fratio":2.1743119266055047,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":17064,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure9-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure9-1.png","fratio":3.2916666666666665,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":189635,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure19-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure19-1.png","fratio":1.2404092071611252,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":109536,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure12-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure12-1.png","fratio":2.1830357142857144,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":69722,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure13-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure13-1.png","fratio":3.4577464788732395,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":29464,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure4-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure4-1.png","fratio":1.8267716535433072,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},{"size":221415,"src":"./images/Deep Network Interpolation for Continuous Imagery Effect Transition-Figure18-1.png","figureid":"Deep Network Interpolation for Continuous Imagery Effect Transition-Figure18-1.png","fratio":0.8546168958742633,"textp":4463143,"paperid":16,"fignums":16,"pagenums":17},],28:[{"size":54970,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure4-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure4-1.png","fratio":0.9623430962343096,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":29696,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure5-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure5-1.png","fratio":1.8125,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":46256,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure6-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure6-1.png","fratio":1.2040816326530612,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":100436,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure2-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure2-1.png","fratio":1.773109243697479,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":14272,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Table2-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Table2-1.png","fratio":3.484375,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":26335,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure8-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure8-1.png","fratio":1.991304347826087,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":128232,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure7-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure7-1.png","fratio":1.7080291970802919,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":42816,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Table1-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Table1-1.png","fratio":4.645833333333333,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},{"size":81656,"src":"./images/DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure3-1.png","figureid":"DF2Net A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction-Figure3-1.png","fratio":2.7283236994219653,"textp":4463143,"paperid":28,"fignums":9,"pagenums":10},],27:[{"size":35850,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure1-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure1-1.png","fratio":1.5933333333333333,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},{"size":25948,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Table1-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Table1-1.png","fratio":9.596153846153847,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},{"size":48510,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure4-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure4-1.png","fratio":4.4,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},{"size":85932,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure5-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure5-1.png","fratio":2.4838709677419355,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},{"size":80192,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure2-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure2-1.png","fratio":2.5027932960893855,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},{"size":63630,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Table2-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Table2-1.png","fratio":4.007936507936508,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},{"size":6106,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Table3-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Table3-1.png","fratio":3.302325581395349,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},{"size":36720,"src":"./images/LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure3-1.png","figureid":"LAP-Net Level-Aware Progressive Network for Image Dehazing-Figure3-1.png","fratio":1.5686274509803921,"textp":4463143,"paperid":27,"fignums":8,"pagenums":10},],13:[{"size":39825,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure1-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure1-1.png","fratio":1.271186440677966,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":44550,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure4-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure4-1.png","fratio":1.1363636363636365,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":86592,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure3-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure3-1.png","fratio":2.7954545454545454,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":18492,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table6-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table6-1.png","fratio":2.1847826086956523,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":21054,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table4-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table4-1.png","fratio":2.781609195402299,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":38394,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table5-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table5-1.png","fratio":1.5379746835443038,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":110920,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure2-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure2-1.png","fratio":2.008510638297872,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":144728,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure5-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Figure5-1.png","fratio":1.4493670886075949,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":7030,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table1-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table1-1.png","fratio":4.868421052631579,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":20184,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table2-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table2-1.png","fratio":2.6666666666666665,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},{"size":7956,"src":"./images/MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table3-1.png","figureid":"MetaCleaner Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition-Table3-1.png","fratio":5.230769230769231,"textp":4463143,"paperid":13,"fignums":11,"pagenums":10},],15:[{"size":12390,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table1-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table1-1.png","fratio":2.5285714285714285,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":13865,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table2-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table2-1.png","fratio":3.983050847457627,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":20406,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure5-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure5-1.png","fratio":1.5701754385964912,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":18675,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table3-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table3-1.png","fratio":2.710843373493976,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":58203,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure2-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure2-1.png","fratio":0.8544061302681992,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":63984,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table4-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table4-1.png","fratio":3.8449612403100777,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":30976,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table5-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Table5-1.png","fratio":1.890625,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":16302,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure3-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure3-1.png","fratio":2.6794871794871793,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":62920,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure6-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure6-1.png","fratio":3.723076923076923,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":39294,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure7-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure7-1.png","fratio":0.7972972972972973,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},{"size":73200,"src":"./images/Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure4-1.png","figureid":"Modulating Image Restoration With Continual Levels via Adaptive Feature Modification Layers-Figure4-1.png","fratio":3.2533333333333334,"textp":4463143,"paperid":15,"fignums":11,"pagenums":9},],17:[{"size":97909,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure7-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure7-1.png","fratio":2.5228426395939088,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":80990,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure1-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure1-1.png","fratio":2.4450549450549453,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":17760,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Table1-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Table1-1.png","fratio":3.2432432432432434,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":13394,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Table2-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Table2-1.png","fratio":2.445945945945946,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":38556,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure2-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure2-1.png","fratio":0.680672268907563,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":89980,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure8-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure8-1.png","fratio":1.8590909090909091,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":25456,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Table3-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Table3-1.png","fratio":4.648648648648648,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":29638,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure4-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure4-1.png","fratio":1.3904109589041096,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":20400,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure3-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure3-1.png","fratio":1.9607843137254901,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":38412,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Table4-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Table4-1.png","fratio":4.082474226804123,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":33200,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure9-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure9-1.png","fratio":1.2048192771084338,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":21109,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure5-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure5-1.png","fratio":2.0693069306930694,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},{"size":29638,"src":"./images/P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure6-1.png","figureid":"P2SGrad Refined Gradients for Optimizing Deep Face Models-Figure6-1.png","fratio":1.3904109589041096,"textp":4463143,"paperid":17,"fignums":13,"pagenums":10},],12:[{"size":48616,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table5-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table5-1.png","fratio":1.145631067961165,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":34944,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Figure1-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Figure1-1.png","fratio":5.743589743589744,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":24816,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table6-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table6-1.png","fratio":1.24822695035461,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":43470,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table7-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table7-1.png","fratio":0.5962962962962963,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":37701,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table8-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table8-1.png","fratio":1.2033898305084745,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":105083,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Figure2-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Figure2-1.png","fratio":1.9356223175965666,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":183456,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Figure4-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Figure4-1.png","fratio":1.1938775510204083,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":29140,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table1-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table1-1.png","fratio":1.8951612903225807,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":56896,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Figure3-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Figure3-1.png","fratio":3.52755905511811,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":19588,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table4-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table4-1.png","fratio":2.8433734939759034,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":47424,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table2-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table2-1.png","fratio":1.0961538461538463,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},{"size":19095,"src":"./images/PA3D Pose-Action 3D Machine for Video Recognition-Table3-1.png","figureid":"PA3D Pose-Action 3D Machine for Video Recognition-Table3-1.png","fratio":2.1157894736842104,"textp":4463143,"paperid":12,"fignums":12,"pagenums":10},],26:[{"size":27432,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure1-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure1-1.png","fratio":1.7007874015748032,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":53816,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table2-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table2-1.png","fratio":3.5,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":139282,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure5-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure5-1.png","fratio":1.7027972027972027,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":14174,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table1-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table1-1.png","fratio":9.81578947368421,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":36842,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure1-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure1-1.png","fratio":3.1009174311926606,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":14350,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table4-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table4-1.png","fratio":2.9285714285714284,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":6840,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table3-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table3-1.png","fratio":5.277777777777778,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":21808,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure6-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure6-1.png","fratio":2.4680851063829787,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":9780,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table5-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table5-1.png","fratio":2.716666666666667,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":124950,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure5-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure5-1.png","fratio":1.9215686274509804,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":59340,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure6-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure6-1.png","fratio":3.1159420289855073,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":103812,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure2-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure2-1.png","fratio":2.3317535545023698,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":52250,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure4-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure4-1.png","fratio":4.318181818181818,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":51012,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure3-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure3-1.png","fratio":4.293577981651376,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":15416,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table6-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table6-1.png","fratio":2.292682926829268,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":16330,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table7-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table7-1.png","fratio":3.23943661971831,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":18400,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure8-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure8-1.png","fratio":2.875,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":7956,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure7-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure7-1.png","fratio":6.882352941176471,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":150698,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table2-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table2-1.png","fratio":1.6523178807947019,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":60465,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure2-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure2-1.png","fratio":3.129496402877698,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":6912,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table1-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Table1-1.png","fratio":5.333333333333333,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":16798,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure4-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure4-1.png","fratio":3.0675675675675675,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},{"size":16632,"src":"./images/RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure3-1.png","figureid":"RankSRGAN Generative Adversarial Networks with Ranker for Image Super-Resolution-Figure3-1.png","fratio":3.2083333333333335,"textp":4463143,"paperid":26,"fignums":23,"pagenums":18},],}